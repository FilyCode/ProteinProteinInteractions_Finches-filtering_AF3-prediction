{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess # Needed to run external commands\n",
    "import tempfile   # Needed to create temporary files\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import metapredict as mpp # For disorder prediction\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing  # For parallelization of metapredict calls\n",
    "import math\n",
    "\n",
    "# Define the absolute path to the cloned s4pred directory\n",
    "s4pred_path = \"/projectnb/cancergrp/Philipp/.conda/pkgs/s4pred\"\n",
    "run_model_script = os.path.join(s4pred_path, \"run_model.py\")\n",
    "\n",
    "# CONSTANTS FOR STANDALONE NETSURFP-3 \n",
    "NETSURFP3_STANDALONE_PATH = \"/projectnb/cancergrp/Philipp/NetSurfP-3.0_standalone\" \n",
    "\n",
    "# The name of the conda environment for the standalone NetSurfP-3 (as created from its environment.yml)\n",
    "NETSURFP3_STANDALONE_ENV_NAME = \"nsp3\" \n",
    "\n",
    "# Full paths to the nsp3.py script and its model file\n",
    "NSP3_SCRIPT_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"nsp3.py\")\n",
    "NSP3_MODEL_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"models\", \"nsp3.pth\")\n",
    "\n",
    "# Batching for parallel NetSurfP-3 calls\n",
    "NETSURFP3_BATCH_SIZE = 50 # Number of proteins to process in each parallel NetSurfP-3.0 call\n",
    "\n",
    "# Netsurfp3 parameter\n",
    "RSA_BURIED_THRESHOLD = 0.25 # Relative Solvent Accessibility threshold: <=0.25 is buried, >0.25 is exposed\n",
    "\n",
    "\n",
    "DATA_DIR = \"/projectnb/cancergrp/Philipp/data/\"\n",
    "RESULTS_DIR = \"/projectnb/cancergrp/Philipp/results/RITA_peptides\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "full_library_df = pd.read_csv(f\"{DATA_DIR}VP_library_all_sequences.csv\")\n",
    "RITA_exp_df = pd.read_excel(f\"{DATA_DIR}RITA_and_ABT_pos_selection_screens.xlsx\", sheet_name='RITA')\n",
    "\n",
    "# Load the full proteins DataFrame\n",
    "full_proteins_df = pd.read_csv(f\"{DATA_DIR}full_library_virus_proteins.csv\")\n",
    "full_proteins_df['NCBI_id'] = full_proteins_df['NCBI_id'].str.split('|').str[0]\n",
    "# Ensure the protein sequence column is named 'Sequence' for consistency\n",
    "if 'Protein Sequence' in full_proteins_df.columns:\n",
    "    full_proteins_df.rename(columns={'Protein Sequence': 'Sequence'}, inplace=True)\n",
    "elif 'sequence' in full_proteins_df.columns: # Check for lowercase 'sequence' too\n",
    "    full_proteins_df.rename(columns={'sequence': 'Sequence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only used to test smaller subsets for debuggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PEPTIDES_FOR_TEST = 200\n",
    "\n",
    "# 1. Select a subset of peptides from the already filtered library\n",
    "test_full_library_filtered = full_library_df.head(N_PEPTIDES_FOR_TEST).copy()\n",
    "\n",
    "# 2. Get the unique NCBI_ids for these test peptides\n",
    "test_ncbi_ids = test_full_library_filtered['NCBI_id'].unique()\n",
    "\n",
    "# 3. Filter the full proteins DataFrame to include only proteins matching these NCBI_ids\n",
    "test_full_proteins_df = full_proteins_df[full_proteins_df['NCBI_id'].isin(test_ncbi_ids)].copy()\n",
    "\n",
    "# 4. Get the unique identifiers (tileID) for these test peptides\n",
    "#    RITA_exp_filtered already has 'identifier' column from 'tileID'\n",
    "test_peptide_identifiers = test_full_library_filtered['identifier'].unique()\n",
    "\n",
    "# 5. Filter the RITA experimental data to include only entries matching these peptide identifiers\n",
    "test_RITA_exp_filtered = RITA_exp_df[RITA_exp_df['tileID'].isin(test_peptide_identifiers)].copy()\n",
    "\n",
    "# --- Replace the original DataFrames with these test subsets ---\n",
    "full_library_df = test_full_library_filtered\n",
    "RITA_exp_df = test_RITA_exp_filtered\n",
    "full_proteins_df = test_full_proteins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Aminoacid distributions for peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 20 standard amino acids for consistent ordering in plots\n",
    "AMINO_ACIDS = sorted(list('ACDEFGHIKLMNPQRSTVWY'))\n",
    "\n",
    "# Helper Function to Calculate Amino Acid Composition \n",
    "def get_amino_acid_composition(sequences):\n",
    "    \"\"\"\n",
    "    Calculates the amino acid composition (percentage) for a list of peptide sequences.\n",
    "    Handles empty sequences or non-string entries gracefully.\n",
    "    \"\"\"\n",
    "    # CORRECTED: Use .empty to check if the pandas Series is empty\n",
    "    if sequences.empty:\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    total_aa_counts = Counter()\n",
    "    total_length = 0\n",
    "    # Filter out non-strings or empty strings before processing\n",
    "    valid_sequences = [s for s in sequences if isinstance(s, str) and s]\n",
    "\n",
    "    # If after filtering, there are no valid sequences, return zeros\n",
    "    if not valid_sequences:\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    for seq in valid_sequences:\n",
    "        total_aa_counts.update(seq)\n",
    "        total_length += len(seq)\n",
    "\n",
    "    if total_length == 0: # This handles cases where valid_sequences might contain only empty strings\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    composition = {aa: (total_aa_counts.get(aa, 0) / total_length) * 100 for aa in AMINO_ACIDS}\n",
    "    return pd.Series(composition, name=\"Composition\")\n",
    "\n",
    "# Helper Function to Plot Amino Acid Composition \n",
    "def plot_composition(composition_series_dict, title, filename_prefix, results_dir):\n",
    "    \"\"\"\n",
    "    Plots amino acid composition for one or more groups using grouped bar plots.\n",
    "    composition_series_dict: dict of {group_name: pandas.Series of composition}\n",
    "    \"\"\"\n",
    "    if not composition_series_dict:\n",
    "        print(f\"Skipping plot '{title}': No data provided.\")\n",
    "        return\n",
    "\n",
    "    # Convert dictionary of Series to a DataFrame for easier plotting\n",
    "    plot_df_data = []\n",
    "    for group_name, series in composition_series_dict.items():\n",
    "        if series is not None and not series.empty: # Ensure series is not None or empty\n",
    "            temp_df = series.reset_index()\n",
    "            temp_df.columns = ['Amino Acid', 'Percentage']\n",
    "            temp_df['Group'] = group_name\n",
    "            plot_df_data.append(temp_df)\n",
    "        else:\n",
    "            print(f\"Warning: No valid composition data for group '{group_name}' in '{title}'.\")\n",
    "\n",
    "    if not plot_df_data:\n",
    "        print(f\"Skipping plot '{title}': No valid dataframes to concatenate.\")\n",
    "        return\n",
    "\n",
    "    plot_df = pd.concat(plot_df_data)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=plot_df, x='Amino Acid', y='Percentage', hue='Group', palette='viridis', ci=None) # ci=None for no confidence intervals as it's aggregated data\n",
    "    plt.title(f'Amino Acid Composition: {title}', fontsize=16)\n",
    "    plt.xlabel('Amino Acid', fontsize=12)\n",
    "    plt.ylabel('Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_dir, f\"{filename_prefix}_amino_acid_composition.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "# Filter DataFrames for VT or VP peptides \n",
    "print(\"\\nFiltering Data \")\n",
    "full_library_filtered = full_library_df[full_library_df['code'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered = RITA_exp_df[RITA_exp_df['type'].isin(['VT', 'VP'])].copy()\n",
    "\n",
    "print(f\"Number of peptides in full library (VT/VP): {len(full_library_filtered)}\")\n",
    "print(f\"Number of peptides used in RITA experiment (VT/VP): {len(RITA_exp_filtered)}\")\n",
    "\n",
    "\n",
    "print(\"\\nGenerating comprehensive peptide amino acid composition and metadata table...\")\n",
    "\n",
    "# Start with the basic peptide info (identifier and sequence) from the filtered full library\n",
    "comprehensive_peptide_table_aa = full_library_filtered[['identifier', 'Aminoacids']].copy()\n",
    "\n",
    "# Calculate amino acid percentages for each individual peptide\n",
    "aa_composition_per_peptide_df = comprehensive_peptide_table_aa['Aminoacids'].apply(lambda seq: pd.Series({\n",
    "    aa: (Counter(seq).get(aa, 0) / len(seq)) * 100 if len(seq) > 0 else 0.0\n",
    "    for aa in AMINO_ACIDS\n",
    "}))\n",
    "\n",
    "# Concatenate the calculated percentages with the initial identifier and Aminoacids columns\n",
    "comprehensive_peptide_table_aa = pd.concat([comprehensive_peptide_table_aa, aa_composition_per_peptide_df], axis=1)\n",
    "\n",
    "# Prepare RITA experiment data for merging\n",
    "RITA_exp_filtered['identifier'] = RITA_exp_filtered['tileID']\n",
    "rita_metadata_for_merge_aa = RITA_exp_filtered[['identifier', 'sig', 'log2FoldChange', 'padj']].copy()\n",
    "\n",
    "# Ensure log2FoldChange and padj are numeric before potential calculations or display\n",
    "rita_metadata_for_merge_aa['log2FoldChange'] = pd.to_numeric(rita_metadata_for_merge_aa['log2FoldChange'], errors='coerce')\n",
    "rita_metadata_for_merge_aa['padj'] = pd.to_numeric(rita_metadata_for_merge_aa['padj'], errors='coerce')\n",
    "\n",
    "\n",
    "# Merge RITA experiment metadata\n",
    "# Use a left merge to ensure all peptides from comprehensive_peptide_table_aa are kept.\n",
    "# Peptides not found in rita_metadata_for_merge_aa (i.e., not used in the experiment)\n",
    "# will have NaN values in the newly merged 'significant', 'log_FC', and 'adj_p_val' columns.\n",
    "comprehensive_peptide_table_aa = comprehensive_peptide_table_aa.merge(\n",
    "    rita_metadata_for_merge_aa,\n",
    "    on='identifier',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the new table\n",
    "print(\"\\nFirst 5 rows of the comprehensive peptide amino acid composition table:\")\n",
    "print(comprehensive_peptide_table_aa.head())\n",
    "print(f\"\\nShape of the comprehensive peptide amino acid composition table: {comprehensive_peptide_table_aa.shape}\")\n",
    "print(f\"Columns in the comprehensive peptide amino acid composition table: {comprehensive_peptide_table_aa.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Save the comprehensive table to a CSV file\n",
    "comprehensive_table_path_aa = os.path.join(RESULTS_DIR, \"comprehensive_peptide_amino_acid_composition_and_metadata.csv\")\n",
    "comprehensive_peptide_table_aa.to_csv(comprehensive_table_path_aa, index=False)\n",
    "print(f\"\\nSaved comprehensive peptide amino acid composition table to: {comprehensive_table_path_aa}\")\n",
    "\n",
    "\n",
    "# Calculate Amino Acid Compositions for Each Group \n",
    "print(\"\\nCalculating Amino Acid Compositions \")\n",
    "\n",
    "# Full Library (VT/VP only)\n",
    "comp_full_library = get_amino_acid_composition(full_library_filtered['Aminoacids'])\n",
    "\n",
    "# Experiment Used (VT/VP only)\n",
    "comp_exp_used = get_amino_acid_composition(RITA_exp_filtered['Aminoacids'])\n",
    "\n",
    "# Peptides Used vs. Not Used in Experiment (from the VT/VP filtered library)\n",
    "used_sequences_set = set(RITA_exp_filtered['Aminoacids'].unique())\n",
    "not_used_peptides_df = full_library_filtered[~full_library_filtered['Aminoacids'].isin(used_sequences_set)]\n",
    "\n",
    "comp_exp_not_used = get_amino_acid_composition(not_used_peptides_df['Aminoacids'])\n",
    "\n",
    "num_used = len(used_sequences_set)\n",
    "num_not_used = len(not_used_peptides_df['Aminoacids'].unique()) # Unique counts for 'not used'\n",
    "print(f\"\\nComparison of Used vs. Not Used peptides (from VT/VP library):\")\n",
    "print(f\"  Total unique peptides in full library (VT/VP): {len(full_library_filtered['Aminoacids'].unique())}\")\n",
    "print(f\"  Unique peptides USED in experiment: {num_used} ({num_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "print(f\"  Unique peptides NOT USED in experiment: {num_not_used} ({num_not_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "\n",
    "\n",
    "# Experiment Significant vs. Non-Significant (from VT/VP used in experiment)\n",
    "RITA_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'Yes']\n",
    "RITA_non_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'No']\n",
    "\n",
    "comp_exp_sig = get_amino_acid_composition(RITA_sig['Aminoacids'])\n",
    "comp_exp_non_sig = get_amino_acid_composition(RITA_non_sig['Aminoacids'])\n",
    "\n",
    "print(f\"\\nSignificant vs. Non-Significant peptides (from VT/VP used in experiment):\")\n",
    "print(f\"  Number of significant peptides: {len(RITA_sig)}\")\n",
    "print(f\"  Number of non-significant peptides: {len(RITA_non_sig)}\")\n",
    "\n",
    "# Experiment Upregulated vs. Downregulated Significant (from VT/VP used and significant)\n",
    "# Ensure log2FoldChange is numeric before comparison\n",
    "RITA_sig['log2FoldChange'] = pd.to_numeric(RITA_sig['log2FoldChange'], errors='coerce')\n",
    "RITA_up = RITA_sig[RITA_sig['log2FoldChange'] > 0]\n",
    "RITA_down = RITA_sig[RITA_sig['log2FoldChange'] < 0]\n",
    "\n",
    "comp_exp_up = get_amino_acid_composition(RITA_up['Aminoacids'])\n",
    "comp_exp_down = get_amino_acid_composition(RITA_down['Aminoacids'])\n",
    "\n",
    "print(f\"\\nUpregulated vs. Downregulated Significant peptides:\")\n",
    "print(f\"  Number of upregulated significant peptides: {len(RITA_up)}\")\n",
    "print(f\"  Number of downregulated significant peptides: {len(RITA_down)}\")\n",
    "print(f\"  Number of significant peptides with logFC = 0 (or NaN): {len(RITA_sig) - len(RITA_up) - len(RITA_down)}\")\n",
    "\n",
    "\n",
    "# Combine Compositions into a Summary DataFrame and Save \n",
    "print(\"\\nGenerating Summary Table \")\n",
    "all_compositions = pd.DataFrame({\n",
    "    'Full_Library_VT_VP': comp_full_library,\n",
    "    'Experiment_Used_VT_VP': comp_exp_used,\n",
    "    'Experiment_Not_Used_VT_VP': comp_exp_not_used,\n",
    "    'Experiment_Significant_VT_VP': comp_exp_sig,\n",
    "    'Experiment_NonSignificant_VT_VP': comp_exp_non_sig,\n",
    "    'Experiment_Upregulated_VT_VP': comp_exp_up,\n",
    "    'Experiment_Downregulated_VT_VP': comp_exp_down\n",
    "})\n",
    "\n",
    "# Round to 2 decimal places for presentation\n",
    "all_compositions = all_compositions.round(2)\n",
    "\n",
    "print(\"\\nAmino Acid Composition Summary (Percentages):\")\n",
    "print(all_compositions)\n",
    "\n",
    "summary_table_path = os.path.join(RESULTS_DIR, \"amino_acid_composition_summary.csv\")\n",
    "all_compositions.to_csv(summary_table_path)\n",
    "print(f\"\\nSaved amino acid composition summary table: {summary_table_path}\")\n",
    "\n",
    "# Plotting the Compositions \n",
    "print(\"\\nGenerating Plots \")\n",
    "\n",
    "# Plot 1: Full Library vs. Experiment Used\n",
    "plot_composition(\n",
    "    {'Full Library (VT/VP)': comp_full_library, 'Experiment Used (VT/VP)': comp_exp_used},\n",
    "    'Full Library vs. Experiment Used Peptides (VT/VP)',\n",
    "    'full_vs_used',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 2: Experiment Used vs. Not Used\n",
    "plot_composition(\n",
    "    {'Experiment Used (VT/VP)': comp_exp_used, 'Experiment Not Used (VT/VP)': comp_exp_not_used},\n",
    "    'Experiment Used vs. Not Used Peptides (VT/VP)',\n",
    "    'used_vs_not_used',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 3: Experiment Significant vs. Non-Significant\n",
    "plot_composition(\n",
    "    {'Significant (VT/VP)': comp_exp_sig, 'Non-Significant (VT/VP)': comp_exp_non_sig},\n",
    "    'Experiment Significant vs. Non-Significant Peptides (VT/VP)',\n",
    "    'significant_vs_nonsignificant',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 4: Experiment Upregulated vs. Downregulated Significant\n",
    "plot_composition(\n",
    "    {'Upregulated Significant (VT/VP)': comp_exp_up, 'Downregulated Significant (VT/VP)': comp_exp_down},\n",
    "    'Upregulated vs. Downregulated Significant Peptides (VT/VP)',\n",
    "    'upregulated_vs_downregulated',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 5: Experiment Subset Comparisons\n",
    "plot_composition(\n",
    "    {'Full Library (VT/VP)': comp_full_library,\n",
    "    'Significant (VT/VP)': comp_exp_sig, \n",
    "    'Non-Significant (VT/VP)': comp_exp_non_sig,\n",
    "    'Upregulated Significant (VT/VP)': comp_exp_up, \n",
    "    'Downregulated Significant (VT/VP)': comp_exp_down},\n",
    "    'Comparison of Peptides (VT/VP)',\n",
    "    'comparison_peptide_amino_acid_comparison',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis Complete! Check your results directory for the summary CSV and plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting structural properties from peptides and information about its location in protein (buried or exposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _extend_protein_sequence(original_seq, target_length):\n",
    "    \"\"\"\n",
    "    Extends a protein sequence by repeating it until it reaches the target_length.\n",
    "    If original_seq is empty, returns an empty string.\n",
    "    If original_seq is already long enough, returns original_seq.\n",
    "    \"\"\"\n",
    "    if len(original_seq) == 0:\n",
    "        return \"\"\n",
    "    if len(original_seq) >= target_length:\n",
    "        return original_seq\n",
    "\n",
    "    # Calculate how many full repeats are needed\n",
    "    num_repeats = math.ceil(target_length / len(original_seq))\n",
    "    extended_seq = (original_seq * num_repeats)\n",
    "    \n",
    "    # Trim if it's longer than needed, but ensure it's at least target_length\n",
    "    return extended_seq[:max(len(original_seq), target_length)]\n",
    "\n",
    "\n",
    "def get_unique_full_protein_info(full_library_proteins_df):\n",
    "    \"\"\"\n",
    "    Extracts unique full protein sequences and their NCBI_ids for NetSurfP-3 input.\n",
    "    Returns a dictionary {NCBI_id: sequence} and a DataFrame suitable for NetsurfP-3 input.\n",
    "    Assumes full_library_proteins_df has 'NCBI_id' and a 'Sequence' column.\n",
    "    \"\"\"\n",
    "    # Ensure 'Sequence' column is string and not empty, and 'NCBI_id' is present\n",
    "    unique_proteins_df = full_library_proteins_df[\n",
    "        full_library_proteins_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "    \n",
    "    # Drop duplicates based on NCBI_id and sequence to ensure unique proteins for prediction\n",
    "    unique_proteins_df = unique_proteins_df.drop_duplicates(subset=['NCBI_id', 'Sequence'])\n",
    "\n",
    "    protein_sequences_dict = unique_proteins_df.set_index('NCBI_id')['Sequence'].to_dict()\n",
    "    # Create a DataFrame for input to NetSurfP-3\n",
    "    protein_input_df = unique_proteins_df[['NCBI_id', 'Sequence']].rename(columns={'NCBI_id': 'identifier', 'Sequence': 'Aminoacids'})\n",
    "    return protein_sequences_dict, protein_input_df\n",
    "\n",
    "# Helper function for buried/exposed prediction with netsurfp3\n",
    "def _run_single_netsurfp3_batch(batch_input_df_args):\n",
    "    \"\"\"\n",
    "    Worker function to run standalone NetSurfP-3 for a single batch of proteins.\n",
    "    Returns a dictionary of {protein_id: [RSA_scores]} for the batch.\n",
    "    \"\"\"\n",
    "    batch_index, batch_df = batch_input_df_args\n",
    "    batch_results = {}\n",
    "    fasta_input_path = None\n",
    "    output_dir = None\n",
    "\n",
    "    KEEP_TEMP_FILES = False \n",
    "\n",
    "    try:\n",
    "        # Check if batch_df is empty after potential subsetting or filtering\n",
    "        if batch_df.empty:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): Input DataFrame for NetSurfP-3 batch is empty. Skipping.\\n\")\n",
    "            return batch_results\n",
    "\n",
    "        # Create temporary FASTA input file for this batch\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=f\"_batch{batch_index}.fasta\") as fasta_file:\n",
    "            fasta_input_path = fasta_file.name\n",
    "            for _, row in batch_df.iterrows():\n",
    "                fasta_file.write(f\">{row['identifier']}\\n{row['Aminoacids']}\\n\")\n",
    "\n",
    "        # Create a temporary output directory for this batch's NetSurfP-3.0 results\n",
    "        output_dir = tempfile.mkdtemp(prefix=f\"nsp3_standalone_output_batch{batch_index}_\")\n",
    "\n",
    "        # Prepare and run the standalone NetSurfP-3.0 script using 'conda run'\n",
    "        command = [\n",
    "            \"conda\", \"run\", \"-n\", NETSURFP3_STANDALONE_ENV_NAME,\n",
    "            \"python\", NSP3_SCRIPT_PATH,\n",
    "            \"-m\", NSP3_MODEL_PATH,\n",
    "            \"-i\", fasta_input_path,\n",
    "            \"-o\", output_dir # Output directory, as per standalone README\n",
    "        ]\n",
    "        \n",
    "        print(f\" (Batch {batch_index}): Running NetSurfP-3 command: {' '.join(command)}\")\n",
    "\n",
    "        # Execute the command, capturing stdout and stderr.\n",
    "        process = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "               \n",
    "        print(f\"DEBUG (Batch {batch_index}): NetSurfP-3 standalone prediction command executed. Checking output directory...\")\n",
    "\n",
    "        # Find the subdirectory created by NetSurfP-3.0 (e.g., '01', '02', etc.)\n",
    "        # This subdirectory contains the actual output files for the batch.\n",
    "        subdirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "        \n",
    "        if not subdirs:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): No subdirectories found in {output_dir}. NetSurfP-3 output structure may have changed.\\n\")\n",
    "            return batch_results # Return empty if no subdirs\n",
    "        \n",
    "        # Assume there's only one relevant subdirectory per batch for now (e.g., '01', '02', etc. per job run)\n",
    "        job_output_subdir_name = subdirs[0] # e.g., '01' based on your debug output\n",
    "        job_output_subdir_path = os.path.join(output_dir, job_output_subdir_name)\n",
    "\n",
    "        # Construct the path to the main CSV output file within that subdirectory.\n",
    "        # Based on your `head 01.csv` output, the file is named after the subdir (e.g., '01.csv').\n",
    "        main_csv_filepath = os.path.join(job_output_subdir_path, f\"{job_output_subdir_name}.csv\") \n",
    "\n",
    "        if not os.path.exists(main_csv_filepath):\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): Expected CSV file '{main_csv_filepath}' not found.\\n\")\n",
    "            sys.stderr.write(f\"Files in {job_output_subdir_path}: {os.listdir(job_output_subdir_path)}\\n\")\n",
    "            return batch_results\n",
    "\n",
    "        protein_rsa_scores_accumulator = {} # Accumulate scores for each protein in this batch\n",
    "        with open(main_csv_filepath, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            # --- CRITICAL FIX: Look for ' rsa' with a leading space and 'id' as key ---\n",
    "            if ' rsa' not in reader.fieldnames: # (yes it needs to have a space, as the developer put spaces in the columns)\n",
    "                sys.stderr.write(f\"Warning (Batch {batch_index}): ' rsa' column not found in {main_csv_filepath}. Found: {reader.fieldnames}. Skipping this file.\\n\")\n",
    "                return batch_results\n",
    "\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # The 'id' column contains '>YP_009944365.1'. Strip the '>' prefix.\n",
    "                    current_protein_id = row['id'].lstrip('>') \n",
    "                    rsa_value = float(row[' rsa']) # Get RSA value from lowercase ' rsa' column (yes it needs to have a space, as the developer put spaces in the columns)\n",
    "                    \n",
    "                    if current_protein_id not in protein_rsa_scores_accumulator:\n",
    "                        protein_rsa_scores_accumulator[current_protein_id] = []\n",
    "                    \n",
    "                    protein_rsa_scores_accumulator[current_protein_id].append(rsa_value)\n",
    "                except (ValueError, KeyError) as parse_e:\n",
    "                    sys.stderr.write(f\"Warning (Batch {batch_index}): Error parsing row in {main_csv_filepath}: {row}. Error: {parse_e}. Skipping row.\\n\")\n",
    "                    continue\n",
    "        \n",
    "        # After processing all rows, populate batch_results\n",
    "        batch_results.update(protein_rsa_scores_accumulator)\n",
    "        \n",
    "        if not batch_results:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): No valid RSA scores were extracted from {main_csv_filepath}.\\n\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        sys.stderr.write(f\"Error running standalone NetSurfP-3 for batch {batch_index} (command: {' '.join(command)}): {e}\\n\")\n",
    "        sys.stderr.write(f\"Stdout (first 500 chars):\\n{e.stdout[:500]}...\\n\")\n",
    "        sys.stderr.write(f\"Stderr:\\n{e.stderr}\\n\")\n",
    "        raise # Re-raise the error as it's critical\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        sys.stderr.write(f\"Error for batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f\"An unexpected error occurred during standalone NetSurfP-3 execution or output parsing for batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up temporary files and directories for this batch\n",
    "        if not KEEP_TEMP_FILES: # Only delete if KEEP_TEMP_FILES is False\n",
    "            if fasta_input_path and os.path.exists(fasta_input_path):\n",
    "                os.remove(fasta_input_path)\n",
    "            if output_dir and os.path.exists(output_dir):\n",
    "                import shutil\n",
    "                shutil.rmtree(output_dir)\n",
    "            \n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def run_netsurfp3_standalone_prediction(input_df, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Runs NetSurfP-3 prediction for a given DataFrame of proteins using the standalone package,\n",
    "    parallelizing by splitting into batches.\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): DataFrame with 'identifier' and 'Aminoacids' columns (for FASTA input).\n",
    "        num_netsurfp3_processes (int, optional): Number of parallel processes to use for NetSurfP-3.\n",
    "                                                If None, uses min(os.cpu_count() or 1, 4).\n",
    "    Returns:\n",
    "        dict: Parsed prediction results: {identifier: [RSA_scores_list]}.\n",
    "              Returns an empty dict if input_df is empty.\n",
    "    Raises:\n",
    "        Exception: If any error occurs during standalone execution or parsing.\n",
    "    \"\"\"\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: No proteins to predict for NetSurfP-3 standalone.\")\n",
    "        return {}\n",
    "\n",
    "    if not os.path.exists(NSP3_SCRIPT_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone script not found: {NSP3_SCRIPT_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH.\")\n",
    "    if not os.path.exists(NSP3_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone model not found: {NSP3_MODEL_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH and 'models/nsp3.pth'.\")\n",
    "    \n",
    "    if num_netsurfp3_processes is None:\n",
    "        num_netsurfp3_processes = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "    # Split the input DataFrame into batches\n",
    "    num_proteins = len(input_df)\n",
    "    num_batches = math.ceil(num_proteins / NETSURFP3_BATCH_SIZE)\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * NETSURFP3_BATCH_SIZE\n",
    "        end_idx = min((i + 1) * NETSURFP3_BATCH_SIZE, num_proteins)\n",
    "        batches.append((i, input_df.iloc[start_idx:end_idx])) # Pass index and batch_df\n",
    "\n",
    "    print(f\"\\nRunning {num_proteins} proteins in {num_batches} batches on {num_netsurfp3_processes} processes for NetSurfP-3 (standalone)...\")\n",
    "    \n",
    "    total_rsa_map = {}\n",
    "    with multiprocessing.Pool(processes=num_netsurfp3_processes) as pool:\n",
    "        # Use imap_unordered for tqdm progress bar and yield results as they complete\n",
    "        for batch_results in tqdm(pool.imap_unordered(_run_single_netsurfp3_batch, batches),\n",
    "                                  total=num_batches,\n",
    "                                  desc=f\"NetSurfP-3 batches ({num_netsurfp3_processes} cores)\"):\n",
    "            total_rsa_map.update(batch_results) # Aggregate results from each batch\n",
    "\n",
    "    return total_rsa_map\n",
    "\n",
    "\n",
    "# WORKER FUNCTION FOR PARALLEL PROCESSING\n",
    "def _process_single_peptide_properties(args):\n",
    "    \"\"\"\n",
    "    Worker function to calculate properties for a single peptide, including buried/exposed status.\n",
    "    This function will be run in parallel processes.\n",
    "    It now expects the *extended* full protein sequence and its corresponding RSA scores\n",
    "    for accurate buried/exposed calculation for short proteins.\n",
    "    \"\"\"\n",
    "    peptide_id, seq, s4pred_pred_string, threshold_disorder, full_protein_seq_extended, protein_rsa_scores_extended, rsa_buried_threshold, ncbi_id = args\n",
    "\n",
    "    initial_props = {f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES}\n",
    "    return_dict = {'identifier': peptide_id, **initial_props}\n",
    "\n",
    "    if len(seq) == 0:\n",
    "        return return_dict\n",
    "\n",
    "    total_residues = len(seq)\n",
    "\n",
    "    # 1. Disorder Prediction (metapredict)\n",
    "    disorder_scores = mpp.predict_disorder(seq)\n",
    "\n",
    "    # Secondary structure prediction string is passed in\n",
    "    if len(s4pred_pred_string) != len(seq):\n",
    "        print(f\"Warning (process {os.getpid()}): s4pred prediction length mismatch for peptide {peptide_id} ({len(s4pred_pred_string)} vs {len(seq)}). Assuming all coil for SS part.\")\n",
    "        s4pred_pred_string = 'C' * len(seq)\n",
    "\n",
    "    # 2. Combine and make mutually exclusive: Disorder (D) overrides SS (H, E, C)\n",
    "    combined_prediction = []\n",
    "    for j in range(len(seq)):\n",
    "        if disorder_scores[j] >= threshold_disorder:\n",
    "            combined_prediction.append('D') # Disordered\n",
    "        else:\n",
    "            combined_prediction.append(s4pred_pred_string[j]) # H, E, or C\n",
    "    combined_prediction_string = \"\".join(combined_prediction)\n",
    "\n",
    "    # 3. Calculate SS and Disorder percentages\n",
    "    return_dict['Disorder_perc'] = (combined_prediction_string.count('D') / total_residues) * 100\n",
    "    return_dict['Helix_perc'] = (combined_prediction_string.count('H') / total_residues) * 100\n",
    "    return_dict['Sheet_perc'] = (combined_prediction_string.count('E') / total_residues) * 100\n",
    "    return_dict['Coil_perc'] = (combined_prediction_string.count('C') / total_residues) * 100\n",
    "\n",
    "    # 4. Buried/Exposed calculation using full protein RSA scores\n",
    "    # Check if we have both the extended full protein sequence and its RSA scores, and if their lengths match.\n",
    "    if full_protein_seq_extended and protein_rsa_scores_extended and len(full_protein_seq_extended) == len(protein_rsa_scores_extended):\n",
    "        \n",
    "        start_index = -1\n",
    "        matched_seq_len = 0\n",
    "        stripped_m_attempted = False\n",
    "\n",
    "        # Attempt 1: Try finding the peptide directly (original sequence)\n",
    "        start_index = full_protein_seq_extended.find(seq)\n",
    "        if start_index != -1:\n",
    "            matched_seq_len = len(seq)\n",
    "        \n",
    "        # Attempt 2: If original not found, and peptide starts with 'M', try without the leading 'M'\n",
    "        if start_index == -1 and seq.startswith('M') and len(seq) > 1:\n",
    "            search_seq_stripped = seq[1:]\n",
    "            start_index = full_protein_seq_extended.find(search_seq_stripped)\n",
    "            if start_index != -1:\n",
    "                matched_seq_len = len(search_seq_stripped)\n",
    "                stripped_m_attempted = True # Mark that stripping was successful\n",
    "        \n",
    "        # If the peptide (or its M-stripped version) was found:\n",
    "        if start_index != -1:\n",
    "            peptide_rsa_slice = protein_rsa_scores_extended[start_index : start_index + matched_seq_len]\n",
    "            \n",
    "            if len(peptide_rsa_slice) == matched_seq_len:\n",
    "                buried_count = sum(1 for rsa in peptide_rsa_slice if rsa < rsa_buried_threshold)\n",
    "                exposed_count = len(peptide_rsa_slice) - buried_count\n",
    "                \n",
    "                return_dict['Buried_perc'] = (buried_count / matched_seq_len) * 100\n",
    "                return_dict['Exposed_perc'] = (exposed_count / matched_seq_len) * 100\n",
    "            else:\n",
    "                sys.stderr.write(f\"Warning: RSA slice length mismatch for peptide '{peptide_id}' (NCBI: {ncbi_id}, seq: '{seq[:20]}...'). \"\n",
    "                                 f\"Expected {matched_seq_len}, got {len(peptide_rsa_slice)}. \"\n",
    "                                 f\"This might indicate an issue with RSA score generation. Buried/Exposed percentages will be 0.\\n\")\n",
    "        else:\n",
    "            reason_attempted = \"\"\n",
    "            if stripped_m_attempted:\n",
    "                reason_attempted = f\" (tried original '{seq[:20]}...' and stripped 'M': '{seq[1:21]}...')\"\n",
    "            elif seq.startswith('M') and len(seq) > 1:\n",
    "                 reason_attempted = f\" (tried original '{seq[:20]}...'; M-stripped failed too)\"\n",
    "            else:\n",
    "                 reason_attempted = f\" (original sequence '{seq[:20]}...')\"\n",
    "\n",
    "            sys.stderr.write(f\"Warning: Peptide '{peptide_id}'{reason_attempted} not found in its extended full protein sequence (NCBI: {ncbi_id}). Buried/Exposed percentages will be 0.\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(f\"Warning: Missing extended full protein sequence or RSA scores for protein '{ncbi_id}' (peptide '{peptide_id}'), \"\n",
    "                         f\"or length mismatch (full_seq len: {len(full_protein_seq_extended) if full_protein_seq_extended else 0} vs rsa scores len: {len(protein_rsa_scores_extended) if protein_rsa_scores_extended else 0}). \"\n",
    "                         f\"Buried/Exposed percentages will be 0.\\n\")\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "# Helper Function to Get Peptide Properties (Disorder, Secondary Structure, Exposed/Buried)\n",
    "def calculate_all_peptide_structural_properties(peptides_df_with_ncbi, full_proteins_df, threshold_disorder=0.5, rsa_buried_threshold=0.25, num_processes=None, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Calculates all peptide properties (Disorder, SS, Buried/Exposed).\n",
    "    Orchestrates full protein RSA prediction and then individual peptide property calculation.\n",
    "    Includes logic to extend short proteins before RSA prediction.\n",
    "    \n",
    "    Args:\n",
    "        peptides_df_with_ncbi (pd.DataFrame): DataFrame with 'identifier', 'Aminoacids', and 'NCBI_id' columns.\n",
    "        full_proteins_df (pd.DataFrame): DataFrame with 'NCBI_id' and 'Sequence' columns for full proteins.\n",
    "        threshold_disorder (float): Disorder score threshold for metapredict.\n",
    "        rsa_buried_threshold (float): RSA threshold for classifying residues as buried.\n",
    "        num_processes (int, optional): Number of CPU cores to use for parallel processing for general peptide properties.\n",
    "        num_netsurfp3_processes (int, optional): Number of CPU cores to use for parallel NetSurfP-3 calls.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame indexed by 'identifier' with all calculated percentage columns.\n",
    "    \"\"\"\n",
    "    if peptides_df_with_ncbi.empty:\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    valid_peptides_df = peptides_df_with_ncbi[\n",
    "        peptides_df_with_ncbi['Aminoacids'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "\n",
    "    if valid_peptides_df.empty:\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    # 1. Get unique full proteins and their *original* sequences\n",
    "    original_full_protein_sequences_dict, full_protein_nsp3_input_df = get_unique_full_protein_info(full_proteins_df)\n",
    "\n",
    "    # Extend short proteins for NetSurfP-3 input\n",
    "    extended_protein_nsp3_input_df_data = []\n",
    "    extended_full_protein_sequences_dict = {} # To store the extended sequences for direct lookup\n",
    "    \n",
    "    # Iterate through unique NCBI_ids to find the max peptide length for each\n",
    "    unique_ncbi_ids = valid_peptides_df['NCBI_id'].unique()\n",
    "    \n",
    "    # Store max peptide length per protein, to ensure protein is at least that long\n",
    "    max_peptide_len_per_protein = {}\n",
    "    for ncbi_id in unique_ncbi_ids:\n",
    "        peptides_for_this_protein = valid_peptides_df[valid_peptides_df['NCBI_id'] == ncbi_id]\n",
    "        if not peptides_for_this_protein.empty:\n",
    "            max_peptide_len_per_protein[ncbi_id] = max(len(p_seq) for p_seq in peptides_for_this_protein['Aminoacids'] if isinstance(p_seq, str))\n",
    "        else:\n",
    "            max_peptide_len_per_protein[ncbi_id] = 0 # Should not happen if filtered correctly\n",
    "            \n",
    "    for ncbi_id, original_seq in original_full_protein_sequences_dict.items():\n",
    "        # The protein needs to be at least as long as its original sequence, AND as long as the longest peptide associated with it\n",
    "        # If no peptides associated (e.g. from filtering), just use original length\n",
    "        required_min_len = max_peptide_len_per_protein.get(ncbi_id, 0)\n",
    "        target_len_for_extension = max(len(original_seq), required_min_len)\n",
    "        \n",
    "        extended_seq = _extend_protein_sequence(original_seq, target_len_for_extension)\n",
    "        extended_full_protein_sequences_dict[ncbi_id] = extended_seq\n",
    "        \n",
    "        extended_protein_nsp3_input_df_data.append({\n",
    "            'identifier': ncbi_id,\n",
    "            'Aminoacids': extended_seq\n",
    "        })\n",
    "        \n",
    "        if len(original_seq) < target_len_for_extension: # Only print debug if extension actually happened\n",
    "            print(f\"DEBUG: Protein {ncbi_id}: Original length {len(original_seq)}, Longest associated peptide {required_min_len}, Extended to {len(extended_seq)}.\")\n",
    "\n",
    "    extended_protein_nsp3_input_df = pd.DataFrame(extended_protein_nsp3_input_df_data)\n",
    "\n",
    "\n",
    "    # 2. Run NetSurfP-3 for RSA on all unique *extended* full proteins (PARALLELIZED)\n",
    "    print(f\"\\nPredicting RSA for {len(extended_protein_nsp3_input_df)} unique (potentially extended) full proteins using NetSurfP-3 (standalone, parallelized)...\")\n",
    "    protein_rsa_map = run_netsurfp3_standalone_prediction(extended_protein_nsp3_input_df, num_netsurfp3_processes=num_netsurfp3_processes)\n",
    "    print(\"NetSurfP-3 RSA prediction complete.\")\n",
    "    \n",
    "    # 3. Run S4PRED for secondary structure on all peptides (same logic as before)\n",
    "    fasta_input_path_s4pred = None\n",
    "    s4pred_output_path = None\n",
    "    s4pred_map = {}\n",
    "\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".fasta\") as fasta_file:\n",
    "            fasta_input_path_s4pred = fasta_file.name\n",
    "            for _, row in valid_peptides_df.iterrows():\n",
    "                fasta_file.write(f\">{row['identifier']}\\n{row['Aminoacids']}\\n\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".ss_fas\") as output_file:\n",
    "            s4pred_output_path = output_file.name\n",
    "            command = [sys.executable, run_model_script, \"--outfmt\", \"fas\", fasta_input_path_s4pred]\n",
    "            print(f\"\\nRunning s4pred via subprocess: {' '.join(command)}\")\n",
    "            try:\n",
    "                subprocess.run(command, check=True, stdout=output_file, stderr=subprocess.PIPE, text=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error running s4pred (command: {' '.join(command)}): {e}\")\n",
    "                print(f\"Stderr: {e.stderr}\")\n",
    "                raise\n",
    "\n",
    "        current_id = None\n",
    "        current_seq_line = None\n",
    "        with open(s4pred_output_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    current_id = line[1:]\n",
    "                    current_seq_line = None\n",
    "                elif current_id and current_seq_line is None:\n",
    "                    current_seq_line = line\n",
    "                elif current_id and current_seq_line is not None:\n",
    "                    s4pred_map[current_id] = line\n",
    "                    current_id = None\n",
    "                    current_seq_line = None\n",
    "\n",
    "    finally:\n",
    "        if fasta_input_path_s4pred and os.path.exists(fasta_input_path_s4pred): os.remove(fasta_input_path_s4pred)\n",
    "        if s4pred_output_path and os.path.exists(s4pred_output_path): os.remove(s4pred_output_path)\n",
    "    # END S4PRED SUBPROCESS BLOCK\n",
    "\n",
    "    # Prepare arguments for parallel processing for each peptide\n",
    "    task_args = []\n",
    "    for _, row in valid_peptides_df.iterrows():\n",
    "        peptide_id = row['identifier']\n",
    "        peptide_seq = row['Aminoacids']\n",
    "        ncbi_id = row['NCBI_id']\n",
    "        \n",
    "        # Now use the extended sequence and RSA scores\n",
    "        full_prot_seq_for_peptide = extended_full_protein_sequences_dict.get(ncbi_id)\n",
    "        prot_rsa_scores_for_peptide = protein_rsa_map.get(ncbi_id)\n",
    "        s4pred_ss = s4pred_map.get(peptide_id, 'C' * len(peptide_seq))\n",
    "\n",
    "        task_args.append(\n",
    "            (peptide_id, peptide_seq, s4pred_ss, threshold_disorder, \n",
    "             full_prot_seq_for_peptide, prot_rsa_scores_for_peptide, rsa_buried_threshold, ncbi_id)\n",
    "        )\n",
    "\n",
    "    print(f\"Calculating peptide properties for {len(valid_peptides_df)} peptides in parallel (disorder overrides SS, plus buried/exposed)...\")\n",
    "\n",
    "    if num_processes is None:\n",
    "        num_processes = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "    results = []\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        for res in tqdm(pool.imap_unordered(_process_single_peptide_properties, task_args),\n",
    "                        total=len(task_args),\n",
    "                        desc=f\"Processing peptides ({num_processes} cores)\"):\n",
    "            results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results).set_index('identifier')\n",
    "\n",
    "\n",
    "# Helper Function to Plot Average Peptide Properties (no changes needed here)\n",
    "def plot_average_properties(average_properties_dict, title, filename_prefix, results_dir):\n",
    "    \"\"\"\n",
    "    Plots average peptide properties (Disorder, Helix, Sheet, Coil) for different groups.\n",
    "    average_properties_dict: dict of {group_name: pandas.Series of average properties}\n",
    "    \"\"\"\n",
    "    if not average_properties_dict:\n",
    "        print(f\"Skipping plot '{title}': No data provided.\")\n",
    "        return\n",
    "\n",
    "    plot_data = []\n",
    "    for group_name, series in average_properties_dict.items():\n",
    "        if series is not None and not series.empty:\n",
    "            # Reorder the series according to PEPTIDE_PROPERTY_TYPES for consistent plotting order\n",
    "            ordered_series = series[[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES]]\n",
    "            temp_df = ordered_series.to_frame(name='Percentage').reset_index()\n",
    "            temp_df.columns = ['Property', 'Percentage']\n",
    "            # Clean up property names for plotting (remove '_perc')\n",
    "            temp_df['Property'] = temp_df['Property'].str.replace('_perc', '')\n",
    "            temp_df['Group'] = group_name\n",
    "            plot_data.append(temp_df)\n",
    "        else:\n",
    "            print(f\"Warning: No valid average property data for group '{group_name}' in '{title}'.\")\n",
    "\n",
    "    if not plot_data:\n",
    "        print(f\"Skipping plot '{title}': No valid dataframes to concatenate.\")\n",
    "        return\n",
    "\n",
    "    plot_df = pd.concat(plot_data)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.barplot(data=plot_df, x='Property', y='Percentage', hue='Group', palette='Spectral', ci=None)\n",
    "    plt.title(f'Average Peptide Properties: {title}', fontsize=16)\n",
    "    plt.xlabel('Peptide Property', fontsize=12)\n",
    "    plt.ylabel('Average Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_dir, f\"{filename_prefix}_peptide_properties.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Filter DataFrames for VT or VP peptides\n",
    "print(\"\\n Filtering Data\")\n",
    "full_library_filtered = full_library_df[full_library_df['code'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered = RITA_exp_df[RITA_exp_df['type'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered['identifier'] = RITA_exp_filtered['tileID']\n",
    "\n",
    "\n",
    "print(f\"Number of peptides in full library (VT/VP): {len(full_library_filtered)}\")\n",
    "print(f\"Number of peptides used in RITA experiment (VT/VP): {len(RITA_exp_filtered)}\")\n",
    "\n",
    "\n",
    "# Peptides Used vs. Not Used in Experiment (from the VT/VP filtered library)\n",
    "used_sequences_set = set(RITA_exp_filtered['Aminoacids'].unique())\n",
    "not_used_peptides_df = full_library_filtered[~full_library_filtered['Aminoacids'].isin(used_sequences_set)].copy()\n",
    "\n",
    "num_used = len(used_sequences_set)\n",
    "num_not_used = len(not_used_peptides_df['Aminoacids'].unique()) # Unique counts for 'not used'\n",
    "print(f\"\\nComparison of Used vs. Not Used peptides (from VT/VP library):\")\n",
    "print(f\"  Total unique peptides in full library (VT/VP): {len(full_library_filtered['Aminoacids'].unique())}\")\n",
    "print(f\"  Unique peptides USED in experiment: {num_used} ({num_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "print(f\"  Unique peptides NOT USED in experiment: {num_not_used} ({num_not_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "\n",
    "\n",
    "# Experiment Significant vs. Non-Significant (from VT/VP used in experiment)\n",
    "RITA_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'Yes'].copy()\n",
    "RITA_non_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'No'].copy()\n",
    "\n",
    "\n",
    "print(f\"\\nSignificant vs. Non-Significant peptides (from VT/VP used in experiment):\")\n",
    "print(f\"  Number of significant peptides: {len(RITA_sig)}\")\n",
    "print(f\"  Number of non-significant peptides: {len(RITA_non_sig)}\")\n",
    "\n",
    "# Experiment Upregulated vs. Downregulated Significant (from VT/VP used and significant)\n",
    "# Ensure log2FoldChange is numeric before comparison\n",
    "RITA_sig['log2FoldChange'] = pd.to_numeric(RITA_sig['log2FoldChange'], errors='coerce')\n",
    "RITA_up = RITA_sig[RITA_sig['log2FoldChange'] > 0].copy()\n",
    "RITA_down = RITA_sig[RITA_sig['log2FoldChange'] < 0].copy()\n",
    "\n",
    "\n",
    "print(f\"\\nUpregulated vs. Downregulated Significant peptides:\")\n",
    "print(f\"  Number of upregulated significant peptides: {len(RITA_up)}\")\n",
    "print(f\"  Number of downregulated significant peptides: {len(RITA_down)}\")\n",
    "print(f\"  Number of significant peptides with logFC = 0 (or NaN): {len(RITA_sig) - len(RITA_up) - len(RITA_down)}\")\n",
    "\n",
    "\n",
    "# Define the peptide property types for consistent ordering in plots.\n",
    "# 'Disorder' will be mutually exclusive with Helix/Sheet/Coil.\n",
    "PEPTIDE_PROPERTY_TYPES = ['Disorder', 'Helix', 'Sheet', 'Coil', 'Buried', 'Exposed']\n",
    "\n",
    "# Calculate all peptide properties once \n",
    "print(\"\\nCalculating all unique peptide properties (Disorder, Secondary Structure) ONCE\")\n",
    "all_peptides_properties_df = calculate_all_peptide_structural_properties(\n",
    "    full_library_filtered[['identifier', 'Aminoacids', 'NCBI_id']],\n",
    "    full_proteins_df,\n",
    "    threshold_disorder=0.5,\n",
    "    rsa_buried_threshold=RSA_BURIED_THRESHOLD,\n",
    "    num_processes=None\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating comprehensive peptide properties and metadata table...\")\n",
    "\n",
    "# Start with the basic peptide info (identifier, sequence, NCBI_id) from the filtered full library\n",
    "comprehensive_peptide_table = full_library_filtered[['identifier', 'Aminoacids', 'NCBI_id']].copy()\n",
    "\n",
    "# Merge with the calculated structural properties from all_peptides_properties_df\n",
    "comprehensive_peptide_table = comprehensive_peptide_table.merge(\n",
    "    all_peptides_properties_df,\n",
    "    left_on='identifier',\n",
    "    right_index=True,\n",
    "    how='left' # Use left merge to keep all peptides from base table\n",
    ")\n",
    "\n",
    "# Prepare RITA experiment data for merging\n",
    "rita_metadata_for_merge = RITA_exp_filtered[['identifier', 'sig', 'log2FoldChange', 'padj']].copy()\n",
    "\n",
    "# Merge RITA experiment metadata\n",
    "comprehensive_peptide_table = comprehensive_peptide_table.merge(\n",
    "    rita_metadata_for_merge,\n",
    "    on='identifier',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the new table\n",
    "print(\"\\nFirst 5 rows of the comprehensive peptide table:\")\n",
    "print(comprehensive_peptide_table.head())\n",
    "print(f\"\\nShape of the comprehensive peptide table: {comprehensive_peptide_table.shape}\")\n",
    "print(f\"Columns in the comprehensive peptide table: {comprehensive_peptide_table.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Save the comprehensive table to a CSV file\n",
    "comprehensive_table_path = os.path.join(RESULTS_DIR, \"comprehensive_peptide_properties_and_metadata_with_RSA.csv\") # Updated filename\n",
    "comprehensive_peptide_table.to_csv(comprehensive_table_path, index=False)\n",
    "print(f\"\\nSaved comprehensive peptide table to: {comprehensive_table_path}\")\n",
    "\n",
    "\n",
    "#  Retrieve Peptide Properties for Each Group from the pre-calculated data \n",
    "print(\"\\nRetrieving and Averaging Peptide Properties for Each Group\")\n",
    "\n",
    "average_properties_series = {}\n",
    "\n",
    "def get_avg_props(identifiers_series, df_source):\n",
    "    if identifiers_series.empty:\n",
    "        return pd.Series({f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES})\n",
    "    unique_ids = identifiers_series.unique()\n",
    "    existing_ids = df_source.index.intersection(unique_ids)\n",
    "    if existing_ids.empty:\n",
    "        print(f\"Warning: No properties found for identifiers in group. Returning zeros for {len(unique_ids)} peptides.\")\n",
    "        return pd.Series({f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES})\n",
    "    \n",
    "    props_df = df_source.loc[existing_ids]\n",
    "    # Ensure only the percentage columns are selected for mean calculation\n",
    "    return props_df[[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES]].mean()\n",
    "\n",
    "\n",
    "average_properties_series['Full_Library_VT_VP'] = get_avg_props(full_library_filtered['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Used_VT_VP'] = get_avg_props(RITA_exp_filtered['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Not_Used_VT_VP'] = get_avg_props(not_used_peptides_df['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Significant_VT_VP'] = get_avg_props(RITA_sig['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_NonSignificant_VT_VP'] = get_avg_props(RITA_non_sig['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Upregulated_VT_VP'] = get_avg_props(RITA_up['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Downregulated_VT_VP'] = get_avg_props(RITA_down['identifier'], all_peptides_properties_df)\n",
    "\n",
    "\n",
    "# Generate and Save Peptide Properties Summary Table\n",
    "print(\"\\nGenerating Peptide Properties Summary Table\")\n",
    "all_peptide_properties_avg = pd.DataFrame(average_properties_series).round(2)\n",
    "\n",
    "print(\"\\nAverage Peptide Properties Summary (Percentages):\")\n",
    "print(all_peptide_properties_avg)\n",
    "properties_summary_table_path = os.path.join(RESULTS_DIR, \"peptide_properties_summary_with_RSA.csv\") # Updated filename\n",
    "all_peptide_properties_avg.to_csv(properties_summary_table_path)\n",
    "print(f\"\\nSaved average peptide properties summary table: {properties_summary_table_path}\")\n",
    "\n",
    "\n",
    "# Plotting the Peptide Properties\n",
    "print(\"\\nGenerating Peptide Properties Plots\")\n",
    "\n",
    "plot_average_properties(\n",
    "    {'Full Library (VT/VP)': average_properties_series['Full_Library_VT_VP'],\n",
    "     'Experiment Used (VT/VP)': average_properties_series['Experiment_Used_VT_VP']},\n",
    "    'Full Library vs. Experiment Used Peptides (VT/VP)',\n",
    "    'full_vs_used_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Experiment Used (VT/VP)': average_properties_series['Experiment_Used_VT_VP'],\n",
    "     'Experiment Not Used (VT/VP)': average_properties_series['Experiment_Not_Used_VT_VP']},\n",
    "    'Experiment Used vs. Not Used Peptides (VT/VP)',\n",
    "    'used_vs_not_used_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Significant (VT/VP)': average_properties_series['Experiment_Significant_VT_VP'],\n",
    "     'Non-Significant (VT/VP)': average_properties_series['Experiment_NonSignificant_VT_VP']},\n",
    "    'Experiment Significant vs. Non-Significant Peptides (VT/VP)',\n",
    "    'significant_vs_nonsignificant_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Upregulated Significant (VT/VP)': average_properties_series['Experiment_Upregulated_VT_VP'],\n",
    "     'Downregulated Significant (VT/VP)': average_properties_series['Experiment_Downregulated_VT_VP']},\n",
    "    'Upregulated vs. Downregulated Significant Peptides (VT/VP)',\n",
    "    'upregulated_vs_downregulated_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "plot_average_properties(\n",
    "    {'Full Library (VT/VP)': average_properties_series['Full_Library_VT_VP'],\n",
    "     'Significant (VT/VP)': average_properties_series['Experiment_Significant_VT_VP'],\n",
    "     'Non-Significant (VT/VP)': average_properties_series['Experiment_NonSignificant_VT_VP'],\n",
    "     'Upregulated Significant (VT/VP)': average_properties_series['Experiment_Upregulated_VT_VP'],\n",
    "     'Downregulated Significant (VT/VP)': average_properties_series['Experiment_Downregulated_VT_VP']},\n",
    "    'Comparison of Peptides (VT/VP)',\n",
    "    'comparison_peptide_properties',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "print(\"\\nAll Analysis Complete! Check your results directory for summary CSVs and plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only used to test a small subset of peptides that make issues with buried/exposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================================================\n",
      "--- Running Focused Buried/Exposed Debug Script with Real Data ---\n",
      "--- Implementing protein sequence extension for short proteins ---\n",
      "==================================================================================\n",
      "\n",
      "[INFO] Loading full_library_df from: /projectnb/cancergrp/Philipp/data/VP_library_all_sequences.csv\n",
      "[INFO] Loading full_proteins_df from: /projectnb/cancergrp/Philipp/data/full_library_virus_proteins.csv\n",
      "[INFO] Initial full_library_df_raw shape: (32742, 9)\n",
      "[INFO] Initial full_proteins_df_raw shape: (1550, 2)\n",
      "\n",
      "[INFO] Found 5 target peptides for analysis.\n",
      "Target Peptides Data:\n",
      "      identifier                                   Aminoacids         NCBI_id\n",
      "342    VT_00343     MSADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAV  YP_009944376.1\n",
      "344    VT_00345              MSADASTFFFFSADASTFFFFSADASTFFFF  YP_009924301.1\n",
      "1076   VT_01077     MSADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAV  YP_009725312.1\n",
      "4666   VT_04667  MSKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLL  YP_009047228.1\n",
      "4769   VT_04770  MAEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIM     NP_579882.1\n",
      "[INFO] Filtered full proteins DataFrame to 5 entries for 5 unique NCBI_ids relevant to target peptides.\n",
      "\n",
      "[DEBUG] Filtering for unique full protein info...\n",
      "[DEBUG] Reduced 5 proteins to 5 unique proteins by NCBI_id and Sequence.\n",
      "[DEBUG] Protein NP_579882.1: Original length 14, Max peptide length for associated peptides 43, Extended to 43.\n",
      "[DEBUG] Protein YP_009047228.1: Original length 14, Max peptide length for associated peptides 43, Extended to 43.\n",
      "[DEBUG] Protein YP_009725312.1: Original length 13, Max peptide length for associated peptides 40, Extended to 40.\n",
      "[DEBUG] Protein YP_009924301.1: Original length 10, Max peptide length for associated peptides 31, Extended to 31.\n",
      "[DEBUG] Protein YP_009944376.1: Original length 13, Max peptide length for associated peptides 40, Extended to 40.\n",
      "\n",
      "[INFO] Running 5 proteins in 1 batches on 2 processes for NetSurfP-3 (standalone)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31a9a010b614c8fbc45a7f22174f889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NetSurfP-3 batches (2 cores):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] NetSurfP-3 RSA prediction complete for relevant proteins (using extended sequences).\n",
      "\n",
      "[INFO] Calculating Buried/Exposed for 5 target peptides...\n",
      "\n",
      "--- Processing Peptide: VT_00343 (NCBI: YP_009944376.1) ---\n",
      "  Peptide Sequence: 'MSADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAV' (Length: 40)\n",
      "  Extended Protein Sequence (preview): 'SADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAVS' (Total Length: 40)\n",
      "  Extended Protein RSA Scores (first 10): [0.8425479531288147, 0.518922746181488, 0.7893075942993164, 0.25048911571502686, 0.39086106419563293, 0.47504130005836487, 0.15299712121486664, 0.18142686784267426, 0.508891224861145, 0.36835891008377075]... (Total: 40)\n",
      "\n",
      "  --- Starting Buried/Exposed Calculation ---\n",
      "    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\n",
      "    [DEBUG] Attempt 1: Searching for original peptide 'MSADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAV' in extended full protein...\n",
      "    [DEBUG] Original peptide 'MSADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAV' NOT found.\n",
      "    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide 'SADASTFLNGFAVSADASTFLNGFAVSADASTFLNGFAV' in extended full protein...\n",
      "    [DEBUG] M-stripped peptide found at index: 0\n",
      "    [DEBUG] Peptide found. Slice extended protein RSA scores from index 0 for length 39.\n",
      "    [DEBUG] Extracted peptide RSA slice (first 10): [0.8425479531288147, 0.518922746181488, 0.7893075942993164, 0.25048911571502686, 0.39086106419563293, 0.47504130005836487, 0.15299712121486664, 0.18142686784267426, 0.508891224861145, 0.36835891008377075]... (Total: 39)\n",
      "    [DEBUG] RSA slice length (39) matches expected matched sequence length (39). Proceeding with buried/exposed calculation.\n",
      "    [DEBUG] Calculated Buried Count: 17\n",
      "    [DEBUG] Calculated Exposed Count: 22\n",
      "\n",
      "  Final Results for VT_00343:\n",
      "    Buried Percentage: 43.59%\n",
      "    Exposed Percentage: 56.41%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Processing Peptide: VT_00345 (NCBI: YP_009924301.1) ---\n",
      "  Peptide Sequence: 'MSADASTFFFFSADASTFFFFSADASTFFFF' (Length: 31)\n",
      "  Extended Protein Sequence (preview): 'SADASTFFFFSADASTFFFFSADASTFFFFS' (Total Length: 31)\n",
      "  Extended Protein RSA Scores (first 10): [0.5035428404808044, 0.3531654477119446, 0.47705069184303284, 0.2796926200389862, 0.21200428903102875, 0.2673299014568329, 0.14386960864067078, 0.12654179334640503, 0.14095275104045868, 0.1368231177330017]... (Total: 31)\n",
      "\n",
      "  --- Starting Buried/Exposed Calculation ---\n",
      "    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\n",
      "    [DEBUG] Attempt 1: Searching for original peptide 'MSADASTFFFFSADASTFFFFSADASTFFFF' in extended full protein...\n",
      "    [DEBUG] Original peptide 'MSADASTFFFFSADASTFFFFSADASTFFFF' NOT found.\n",
      "    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide 'SADASTFFFFSADASTFFFFSADASTFFFF' in extended full protein...\n",
      "    [DEBUG] M-stripped peptide found at index: 0\n",
      "    [DEBUG] Peptide found. Slice extended protein RSA scores from index 0 for length 30.\n",
      "    [DEBUG] Extracted peptide RSA slice (first 10): [0.5035428404808044, 0.3531654477119446, 0.47705069184303284, 0.2796926200389862, 0.21200428903102875, 0.2673299014568329, 0.14386960864067078, 0.12654179334640503, 0.14095275104045868, 0.1368231177330017]... (Total: 30)\n",
      "    [DEBUG] RSA slice length (30) matches expected matched sequence length (30). Proceeding with buried/exposed calculation.\n",
      "    [DEBUG] Calculated Buried Count: 16\n",
      "    [DEBUG] Calculated Exposed Count: 14\n",
      "\n",
      "  Final Results for VT_00345:\n",
      "    Buried Percentage: 53.33%\n",
      "    Exposed Percentage: 46.67%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Processing Peptide: VT_01077 (NCBI: YP_009725312.1) ---\n",
      "  Peptide Sequence: 'MSADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAV' (Length: 40)\n",
      "  Extended Protein Sequence (preview): 'SADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAVS' (Total Length: 40)\n",
      "  Extended Protein RSA Scores (first 10): [0.8272184729576111, 0.6118466854095459, 0.7436037659645081, 0.17805205285549164, 0.5163078308105469, 0.5791890621185303, 0.2646104693412781, 0.20864781737327576, 0.5822922587394714, 0.4223787784576416]... (Total: 40)\n",
      "\n",
      "  --- Starting Buried/Exposed Calculation ---\n",
      "    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\n",
      "    [DEBUG] Attempt 1: Searching for original peptide 'MSADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAV' in extended full protein...\n",
      "    [DEBUG] Original peptide 'MSADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAV' NOT found.\n",
      "    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide 'SADAQSFLNGFAVSADAQSFLNGFAVSADAQSFLNGFAV' in extended full protein...\n",
      "    [DEBUG] M-stripped peptide found at index: 0\n",
      "    [DEBUG] Peptide found. Slice extended protein RSA scores from index 0 for length 39.\n",
      "    [DEBUG] Extracted peptide RSA slice (first 10): [0.8272184729576111, 0.6118466854095459, 0.7436037659645081, 0.17805205285549164, 0.5163078308105469, 0.5791890621185303, 0.2646104693412781, 0.20864781737327576, 0.5822922587394714, 0.4223787784576416]... (Total: 39)\n",
      "    [DEBUG] RSA slice length (39) matches expected matched sequence length (39). Proceeding with buried/exposed calculation.\n",
      "    [DEBUG] Calculated Buried Count: 15\n",
      "    [DEBUG] Calculated Exposed Count: 24\n",
      "\n",
      "  Final Results for VT_01077:\n",
      "    Buried Percentage: 38.46%\n",
      "    Exposed Percentage: 61.54%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Processing Peptide: VT_04667 (NCBI: YP_009047228.1) ---\n",
      "  Peptide Sequence: 'MSKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLL' (Length: 43)\n",
      "  Extended Protein Sequence (preview): 'SKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLLS' (Total Length: 43)\n",
      "  Extended Protein RSA Scores (first 10): [0.8313835263252258, 0.7215954661369324, 0.7322922348976135, 0.5455408096313477, 0.6766543984413147, 0.31519070267677307, 0.26531317830085754, 0.6389404535293579, 0.5463358759880066, 0.5789359211921692]... (Total: 43)\n",
      "\n",
      "  --- Starting Buried/Exposed Calculation ---\n",
      "    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\n",
      "    [DEBUG] Attempt 1: Searching for original peptide 'MSKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLL' in extended full protein...\n",
      "    [DEBUG] Original peptide 'MSKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLL' NOT found.\n",
      "    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide 'SKDSNFLNESGVLLSKDSNFLNESGVLLSKDSNFLNESGVLL' in extended full protein...\n",
      "    [DEBUG] M-stripped peptide found at index: 0\n",
      "    [DEBUG] Peptide found. Slice extended protein RSA scores from index 0 for length 42.\n",
      "    [DEBUG] Extracted peptide RSA slice (first 10): [0.8313835263252258, 0.7215954661369324, 0.7322922348976135, 0.5455408096313477, 0.6766543984413147, 0.31519070267677307, 0.26531317830085754, 0.6389404535293579, 0.5463358759880066, 0.5789359211921692]... (Total: 42)\n",
      "    [DEBUG] RSA slice length (42) matches expected matched sequence length (42). Proceeding with buried/exposed calculation.\n",
      "    [DEBUG] Calculated Buried Count: 7\n",
      "    [DEBUG] Calculated Exposed Count: 35\n",
      "\n",
      "  Final Results for VT_04667:\n",
      "    Buried Percentage: 16.67%\n",
      "    Exposed Percentage: 83.33%\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Processing Peptide: VT_04770 (NCBI: NP_579882.1) ---\n",
      "  Peptide Sequence: 'MAEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIM' (Length: 43)\n",
      "  Extended Protein Sequence (preview): 'AEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIMA' (Total Length: 43)\n",
      "  Extended Protein RSA Scores (first 10): [0.8377313613891602, 0.7665957808494568, 0.43324682116508484, 0.41953420639038086, 0.6166459321975708, 0.7412523627281189, 0.3789790868759155, 0.6766311526298523, 0.6642792820930481, 0.5361850261688232]... (Total: 43)\n",
      "\n",
      "  --- Starting Buried/Exposed Calculation ---\n",
      "    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\n",
      "    [DEBUG] Attempt 1: Searching for original peptide 'MAEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIM' in extended full protein...\n",
      "    [DEBUG] Original peptide 'MAEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIM' NOT found.\n",
      "    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide 'AEAMSQVTNSATIMAEAMSQVTNSATIMAEAMSQVTNSATIM' in extended full protein...\n",
      "    [DEBUG] M-stripped peptide found at index: 0\n",
      "    [DEBUG] Peptide found. Slice extended protein RSA scores from index 0 for length 42.\n",
      "    [DEBUG] Extracted peptide RSA slice (first 10): [0.8377313613891602, 0.7665957808494568, 0.43324682116508484, 0.41953420639038086, 0.6166459321975708, 0.7412523627281189, 0.3789790868759155, 0.6766311526298523, 0.6642792820930481, 0.5361850261688232]... (Total: 42)\n",
      "    [DEBUG] RSA slice length (42) matches expected matched sequence length (42). Proceeding with buried/exposed calculation.\n",
      "    [DEBUG] Calculated Buried Count: 12\n",
      "    [DEBUG] Calculated Exposed Count: 30\n",
      "\n",
      "  Final Results for VT_04770:\n",
      "    Buried Percentage: 28.57%\n",
      "    Exposed Percentage: 71.43%\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================================================\n",
      "--- Final Buried/Exposed Results for Target Peptides ---\n",
      "==================================================================================\n",
      "            Buried_perc  Exposed_perc\n",
      "identifier                           \n",
      "VT_00343      43.589744     56.410256\n",
      "VT_00345      53.333333     46.666667\n",
      "VT_01077      38.461538     61.538462\n",
      "VT_04667      16.666667     83.333333\n",
      "VT_04770      28.571429     71.428571\n",
      "\n",
      "[INFO] Script finished. Examine the debug output above for details on each peptide.\n",
      "If 'Buried_perc' and 'Exposed_perc' are 0.00% for a peptide, look for 'Warning:' messages above.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess # Needed to run external commands\n",
    "import tempfile   # Needed to create temporary files\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import multiprocessing \n",
    "import math\n",
    "from tqdm.auto import tqdm # Keep tqdm for NetSurfP-3 batch progress\n",
    "\n",
    "# --- Configuration & Constants from your original script ---\n",
    "NETSURFP3_STANDALONE_PATH = \"/projectnb/cancergrp/Philipp/NetSurfP-3.0_standalone\" \n",
    "NETSURFP3_STANDALONE_ENV_NAME = \"nsp3\" \n",
    "NSP3_SCRIPT_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"nsp3.py\")\n",
    "NSP3_MODEL_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"models\", \"nsp3.pth\")\n",
    "NETSURFP3_BATCH_SIZE = 50 \n",
    "RSA_BURIED_THRESHOLD = 0.25 \n",
    "\n",
    "DATA_DIR = \"/projectnb/cancergrp/Philipp/data/\"\n",
    "\n",
    "PEPTIDE_PROPERTY_TYPES = ['Buried', 'Exposed'] \n",
    "\n",
    "# --- Problematic Peptides to Focus On ---\n",
    "TARGET_PEPTIDE_IDS = [\n",
    "    'VT_00343',\n",
    "    'VT_00345',\n",
    "    'VT_01077',\n",
    "    'VT_04667',\n",
    "    'VT_04770',\n",
    "    # Add any other specific problematic peptide IDs here\n",
    "]\n",
    "\n",
    "# --- Functions directly extracted and adapted from your original script ---\n",
    "\n",
    "def get_unique_full_protein_info(full_library_proteins_df):\n",
    "    \"\"\"\n",
    "    Extracts unique full protein sequences and their NCBI_ids for NetSurfP-3 input.\n",
    "    Returns a dictionary {NCBI_id: sequence} and a DataFrame suitable for NetsurfP-3 input.\n",
    "    Assumes full_library_proteins_df has 'NCBI_id' and a 'Sequence' column.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[DEBUG] Filtering for unique full protein info...\")\n",
    "    unique_proteins_df = full_library_proteins_df[\n",
    "        full_library_proteins_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "    \n",
    "    initial_count = len(unique_proteins_df)\n",
    "    unique_proteins_df = unique_proteins_df.drop_duplicates(subset=['NCBI_id', 'Sequence'])\n",
    "    print(f\"[DEBUG] Reduced {initial_count} proteins to {len(unique_proteins_df)} unique proteins by NCBI_id and Sequence.\")\n",
    "\n",
    "    protein_sequences_dict = unique_proteins_df.set_index('NCBI_id')['Sequence'].to_dict()\n",
    "    protein_input_df = unique_proteins_df[['NCBI_id', 'Sequence']].rename(columns={'NCBI_id': 'identifier', 'Sequence': 'Aminoacids'})\n",
    "    return protein_sequences_dict, protein_input_df\n",
    "\n",
    "def _run_single_netsurfp3_batch(batch_input_df_args):\n",
    "    \"\"\"\n",
    "    Worker function to run standalone NetSurfP-3 for a single batch of proteins.\n",
    "    Returns a dictionary of {protein_id: [RSA_scores]} for the batch.\n",
    "    \"\"\"\n",
    "    batch_index, batch_df = batch_input_df_args\n",
    "    batch_results = {}\n",
    "    fasta_input_path = None\n",
    "    output_dir = None\n",
    "\n",
    "    # Set to True for debugging NetSurfP-3 issues (will leave temp files)\n",
    "    KEEP_TEMP_FILES_NSP3 = False \n",
    "\n",
    "    try:\n",
    "        if batch_df.empty:\n",
    "            sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): Input DataFrame for NetSurfP-3 batch is empty. Skipping.\\n\")\n",
    "            return batch_results\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=f\"_batch{batch_index}.fasta\") as fasta_file:\n",
    "            fasta_input_path = fasta_file.name\n",
    "            for _, row in batch_df.iterrows():\n",
    "                fasta_file.write(f\">{row['identifier']}\\n{row['Aminoacids']}\\n\")\n",
    "\n",
    "        output_dir = tempfile.mkdtemp(prefix=f\"nsp3_standalone_output_batch{batch_index}_\")\n",
    "\n",
    "        command = [\n",
    "            \"conda\", \"run\", \"-n\", NETSURFP3_STANDALONE_ENV_NAME,\n",
    "            \"python\", NSP3_SCRIPT_PATH,\n",
    "            \"-m\", NSP3_MODEL_PATH,\n",
    "            \"-i\", fasta_input_path,\n",
    "            \"-o\", output_dir\n",
    "        ]\n",
    "        \n",
    "        # print(f\"[DEBUG] (NetSurfP-3 Batch {batch_index}): Running NetSurfP-3 command: {' '.join(command)}\")\n",
    "        process = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "               \n",
    "        # print(f\"[DEBUG] (NetSurfP-3 Batch {batch_index}): NetSurfP-3 standalone prediction command executed. Checking output directory...\")\n",
    "\n",
    "        subdirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "        \n",
    "        if not subdirs:\n",
    "            sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): No subdirectories found in {output_dir}. NetSurfP-3 output structure may have changed or failed silently.\\n\")\n",
    "            # print(f\"DEBUG: Stderr from NetSurfP-3 process (Batch {batch_index}):\\n{process.stderr}\\n\") # More stderr debug\n",
    "            return batch_results\n",
    "        \n",
    "        job_output_subdir_name = subdirs[0]\n",
    "        job_output_subdir_path = os.path.join(output_dir, job_output_subdir_name)\n",
    "        main_csv_filepath = os.path.join(job_output_subdir_path, f\"{job_output_subdir_name}.csv\") \n",
    "\n",
    "        if not os.path.exists(main_csv_filepath):\n",
    "            sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): Expected CSV file '{main_csv_filepath}' not found.\\n\")\n",
    "            sys.stderr.write(f\"Files in {job_output_subdir_path}: {os.listdir(job_output_subdir_path)}\\n\")\n",
    "            # print(f\"DEBUG: Stderr from NetSurfP-3 process (Batch {batch_index}):\\n{process.stderr}\\n\") # More stderr debug\n",
    "            return batch_results\n",
    "\n",
    "        protein_rsa_scores_accumulator = {}\n",
    "        with open(main_csv_filepath, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            if ' rsa' not in reader.fieldnames:\n",
    "                sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): ' rsa' column not found in {main_csv_filepath}. Found: {reader.fieldnames}. Skipping this file.\\n\")\n",
    "                return batch_results\n",
    "\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    current_protein_id = row['id'].lstrip('>') \n",
    "                    rsa_value = float(row[' rsa'])\n",
    "                    \n",
    "                    if current_protein_id not in protein_rsa_scores_accumulator:\n",
    "                        protein_rsa_scores_accumulator[current_protein_id] = []\n",
    "                    \n",
    "                    protein_rsa_scores_accumulator[current_protein_id].append(rsa_value)\n",
    "                except (ValueError, KeyError) as parse_e:\n",
    "                    sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): Error parsing row in {main_csv_filepath}: {row}. Error: {parse_e}. Skipping row.\\n\")\n",
    "                    continue\n",
    "        \n",
    "        batch_results.update(protein_rsa_scores_accumulator)\n",
    "        \n",
    "        if not batch_results:\n",
    "            sys.stderr.write(f\"Warning (NetSurfP-3 Batch {batch_index}): No valid RSA scores were extracted from {main_csv_filepath}.\\n\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        sys.stderr.write(f\"Error running standalone NetSurfP-3 for batch {batch_index} (command: {' '.join(command)}): {e}\\n\")\n",
    "        sys.stderr.write(f\"Stdout (first 500 chars):\\n{e.stdout[:500]}...\\n\")\n",
    "        sys.stderr.write(f\"Stderr:\\n{e.stderr}\\n\")\n",
    "        raise\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        sys.stderr.write(f\"Error for NetSurfP-3 batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f\"An unexpected error occurred during standalone NetSurfP-3 execution or output parsing for batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    finally:\n",
    "        if not KEEP_TEMP_FILES_NSP3:\n",
    "            if fasta_input_path and os.path.exists(fasta_input_path):\n",
    "                os.remove(fasta_input_path)\n",
    "            if output_dir and os.path.exists(output_dir):\n",
    "                import shutil\n",
    "                shutil.rmtree(output_dir)\n",
    "            \n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def run_netsurfp3_standalone_prediction(input_df, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Runs NetSurfP-3 prediction for a given DataFrame of proteins using the standalone package,\n",
    "    parallelizing by splitting into batches.\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): DataFrame with 'identifier' and 'Aminoacids' columns (for FASTA input).\n",
    "        num_netsurfp3_processes (int, optional): Number of parallel processes to use for NetSurfP-3.\n",
    "                                                If None, uses min(os.cpu_count() or 1, 4).\n",
    "    Returns:\n",
    "        dict: Parsed prediction results: {identifier: [RSA_scores_list]}.\n",
    "              Returns an empty dict if input_df is empty.\n",
    "    Raises:\n",
    "        Exception: If any error occurs during standalone execution or parsing.\n",
    "    \"\"\"\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: No proteins to predict for NetSurfP-3 standalone.\")\n",
    "        return {}\n",
    "\n",
    "    if not os.path.exists(NSP3_SCRIPT_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone script not found: {NSP3_SCRIPT_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH.\")\n",
    "    if not os.path.exists(NSP3_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone model not found: {NSP3_MODEL_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH and 'models/nsp3.pth'.\")\n",
    "    \n",
    "    if num_netsurfp3_processes is None:\n",
    "        num_netsurfp3_processes = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "    num_proteins = len(input_df)\n",
    "    num_batches = math.ceil(num_proteins / NETSURFP3_BATCH_SIZE)\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * NETSURFP3_BATCH_SIZE\n",
    "        end_idx = min((i + 1) * NETSURFP3_BATCH_SIZE, num_proteins)\n",
    "        batches.append((i, input_df.iloc[start_idx:end_idx]))\n",
    "\n",
    "    print(f\"\\n[INFO] Running {num_proteins} proteins in {num_batches} batches on {num_netsurfp3_processes} processes for NetSurfP-3 (standalone)...\")\n",
    "    \n",
    "    total_rsa_map = {}\n",
    "    with multiprocessing.Pool(processes=num_netsurfp3_processes) as pool:\n",
    "        for batch_results in tqdm(pool.imap_unordered(_run_single_netsurfp3_batch, batches),\n",
    "                                  total=num_batches,\n",
    "                                  desc=f\"NetSurfP-3 batches ({num_netsurfp3_processes} cores)\"):\n",
    "            total_rsa_map.update(batch_results)\n",
    "\n",
    "    return total_rsa_map\n",
    "\n",
    "\n",
    "def _process_single_peptide_buried_exposed(peptide_id, seq, full_protein_seq_extended, protein_rsa_scores_extended, rsa_buried_threshold, ncbi_id):\n",
    "    \"\"\"\n",
    "    Worker function to calculate buried/exposed status for a single peptide, with extensive debug prints.\n",
    "    It now expects an *extended* full_protein_seq and its corresponding *extended* protein_rsa_scores.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing Peptide: {peptide_id} (NCBI: {ncbi_id}) ---\")\n",
    "    print(f\"  Peptide Sequence: '{seq}' (Length: {len(seq)})\")\n",
    "    \n",
    "    protein_seq_preview = full_protein_seq_extended[:100] + '...' if full_protein_seq_extended and len(full_protein_seq_extended) > 100 else full_protein_seq_extended\n",
    "    print(f\"  Extended Protein Sequence (preview): '{protein_seq_preview}' (Total Length: {len(full_protein_seq_extended) if full_protein_seq_extended else 0})\")\n",
    "    \n",
    "    rsa_scores_preview = protein_rsa_scores_extended[:10] if protein_rsa_scores_extended else \"N/A\" \n",
    "    print(f\"  Extended Protein RSA Scores (first 10): {rsa_scores_preview}... (Total: {len(protein_rsa_scores_extended) if protein_rsa_scores_extended else 0})\")\n",
    "    \n",
    "    return_dict = {\n",
    "        'identifier': peptide_id,\n",
    "        'Buried_perc': 0.0,\n",
    "        'Exposed_perc': 0.0\n",
    "    }\n",
    "\n",
    "    if len(seq) == 0:\n",
    "        print(f\"    [DEBUG] Peptide '{peptide_id}' has an empty sequence. Skipping calculation.\")\n",
    "        return return_dict\n",
    "\n",
    "    print(\"\\n  --- Starting Buried/Exposed Calculation ---\")\n",
    "    \n",
    "    # Initial check for full protein sequence and RSA scores presence and length match\n",
    "    if not full_protein_seq_extended:\n",
    "        sys.stderr.write(f\"    Warning: Extended full protein sequence is missing for NCBI '{ncbi_id}' (peptide '{peptide_id}'). Buried/Exposed percentages will be 0.\\n\")\n",
    "    elif not protein_rsa_scores_extended:\n",
    "        sys.stderr.write(f\"    Warning: Extended RSA scores are missing for protein '{ncbi_id}' (peptide '{peptide_id}'). Buried/Exposed percentages will be 0.\\n\")\n",
    "    elif full_protein_seq_extended and protein_rsa_scores_extended and len(full_protein_seq_extended) != len(protein_rsa_scores_extended):\n",
    "        sys.stderr.write(f\"    Warning: Length mismatch between extended full protein sequence ({len(full_protein_seq_extended)}) and extended RSA scores ({len(protein_rsa_scores_extended)}) for NCBI '{ncbi_id}' (peptide '{peptide_id}'). Buried/Exposed percentages will be 0.\\n\")\n",
    "    \n",
    "    if full_protein_seq_extended and protein_rsa_scores_extended and len(full_protein_seq_extended) == len(protein_rsa_scores_extended):\n",
    "        print(\"    [DEBUG] Initial check: Extended full protein sequence and RSA scores are present and lengths match.\")\n",
    "        \n",
    "        start_index = -1\n",
    "        matched_seq_len = 0\n",
    "        stripped_m_attempted = False\n",
    "\n",
    "        # Attempt 1: Try finding the peptide directly (original sequence)\n",
    "        print(f\"    [DEBUG] Attempt 1: Searching for original peptide '{seq}' in extended full protein...\")\n",
    "        start_index = full_protein_seq_extended.find(seq)\n",
    "        if start_index != -1:\n",
    "            matched_seq_len = len(seq)\n",
    "            print(f\"    [DEBUG] Original peptide found at index: {start_index}\")\n",
    "        else:\n",
    "            print(f\"    [DEBUG] Original peptide '{seq}' NOT found.\")\n",
    "        \n",
    "        # Attempt 2: If original not found, and peptide starts with 'M', try without the leading 'M'\n",
    "        if start_index == -1 and seq.startswith('M') and len(seq) > 1:\n",
    "            search_seq_stripped = seq[1:]\n",
    "            print(f\"    [DEBUG] Attempt 2: Original not found, and peptide starts with 'M'. Searching for M-stripped peptide '{search_seq_stripped}' in extended full protein...\")\n",
    "            start_index = full_protein_seq_extended.find(search_seq_stripped)\n",
    "            if start_index != -1:\n",
    "                matched_seq_len = len(search_seq_stripped)\n",
    "                stripped_m_attempted = True\n",
    "                print(f\"    [DEBUG] M-stripped peptide found at index: {start_index}\")\n",
    "            else:\n",
    "                print(f\"    [DEBUG] M-stripped peptide '{search_seq_stripped}' NOT found either.\")\n",
    "        \n",
    "        if start_index != -1:\n",
    "            print(f\"    [DEBUG] Peptide found. Slice extended protein RSA scores from index {start_index} for length {matched_seq_len}.\")\n",
    "            peptide_rsa_slice = protein_rsa_scores_extended[start_index : start_index + matched_seq_len]\n",
    "            \n",
    "            rsa_slice_preview = peptide_rsa_slice[:10] if peptide_rsa_slice else \"N/A\"\n",
    "            print(f\"    [DEBUG] Extracted peptide RSA slice (first 10): {rsa_slice_preview}... (Total: {len(peptide_rsa_slice)})\")\n",
    "\n",
    "            if len(peptide_rsa_slice) == matched_seq_len:\n",
    "                print(f\"    [DEBUG] RSA slice length ({len(peptide_rsa_slice)}) matches expected matched sequence length ({matched_seq_len}). Proceeding with buried/exposed calculation.\")\n",
    "                buried_count = sum(1 for rsa in peptide_rsa_slice if rsa < rsa_buried_threshold)\n",
    "                exposed_count = len(peptide_rsa_slice) - buried_count\n",
    "                \n",
    "                return_dict['Buried_perc'] = (buried_count / matched_seq_len) * 100\n",
    "                return_dict['Exposed_perc'] = (exposed_count / matched_seq_len) * 100\n",
    "                \n",
    "                print(f\"    [DEBUG] Calculated Buried Count: {buried_count}\")\n",
    "                print(f\"    [DEBUG] Calculated Exposed Count: {exposed_count}\")\n",
    "            else:\n",
    "                sys.stderr.write(f\"    Warning: RSA slice length mismatch for peptide '{peptide_id}' (NCBI: {ncbi_id}, seq: '{seq[:20]}...'). \"\n",
    "                                 f\"Expected {matched_seq_len}, got {len(peptide_rsa_slice)}. \"\n",
    "                                 f\"This might indicate an issue with RSA score generation. Buried/Exposed percentages will be 0.\\n\")\n",
    "        else:\n",
    "            reason_attempted = \"\"\n",
    "            if stripped_m_attempted:\n",
    "                reason_attempted = f\" (tried original '{seq[:20]}...' and stripped 'M': '{seq[1:21]}...')\"\n",
    "            elif seq.startswith('M') and len(seq) > 1:\n",
    "                 reason_attempted = f\" (tried original '{seq[:20]}...'; M-stripped failed too)\"\n",
    "            else:\n",
    "                 reason_attempted = f\" (original sequence '{seq[:20]}...')\"\n",
    "\n",
    "            sys.stderr.write(f\"    Warning: Peptide '{peptide_id}'{reason_attempted} not found in its extended full protein sequence (NCBI: {ncbi_id}). Buried/Exposed percentages will be 0.\\n\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print(f\"\\n  Final Results for {peptide_id}:\")\n",
    "    print(f\"    Buried Percentage: {return_dict['Buried_perc']:.2f}%\")\n",
    "    print(f\"    Exposed Percentage: {return_dict['Exposed_perc']:.2f}%\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def _extend_protein_sequence(original_seq, target_length):\n",
    "    \"\"\"\n",
    "    Extends a protein sequence by repeating it until it reaches the target_length.\n",
    "    \"\"\"\n",
    "    if len(original_seq) == 0:\n",
    "        return \"\"\n",
    "    if len(original_seq) >= target_length:\n",
    "        return original_seq\n",
    "\n",
    "    num_repeats = math.ceil(target_length / len(original_seq))\n",
    "    extended_seq = (original_seq * num_repeats)\n",
    "    \n",
    "    # Trim if it's longer than needed, but ensure it's at least target_length\n",
    "    return extended_seq[:max(len(original_seq), target_length)]\n",
    "\n",
    "def calculate_filtered_peptide_properties(peptides_df, full_proteins_df, rsa_buried_threshold, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Calculates only Buried/Exposed properties for a filtered set of peptides.\n",
    "    Orchestrates full protein RSA prediction and then individual peptide property calculation.\n",
    "    Includes logic to extend short proteins before RSA prediction.\n",
    "    \"\"\"\n",
    "    if peptides_df.empty:\n",
    "        print(\"[INFO] No peptides to process.\")\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    valid_peptides_df = peptides_df[\n",
    "        peptides_df['Aminoacids'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "\n",
    "    if valid_peptides_df.empty:\n",
    "        print(\"[INFO] No valid peptides (empty or non-string sequences) to process.\")\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    # Ensure full_proteins_df only contains proteins relevant to our filtered peptides\n",
    "    relevant_ncbi_ids = valid_peptides_df['NCBI_id'].unique()\n",
    "    filtered_full_proteins_df = full_proteins_df[full_proteins_df['NCBI_id'].isin(relevant_ncbi_ids)].copy()\n",
    "    print(f\"[INFO] Filtered full proteins DataFrame to {len(filtered_full_proteins_df)} entries for {len(relevant_ncbi_ids)} unique NCBI_ids relevant to target peptides.\")\n",
    "\n",
    "    # 1. Get unique full proteins and their sequences for NetSurfP-3 input (original versions)\n",
    "    original_full_protein_sequences_dict, full_protein_nsp3_input_df = get_unique_full_protein_info(filtered_full_proteins_df)\n",
    "\n",
    "    # --- NEW LOGIC: Extend short proteins for RSA prediction ---\n",
    "    extended_protein_nsp3_input_df_data = []\n",
    "    extended_full_protein_sequences_dict = {} # To store the extended sequences for direct lookup\n",
    "    \n",
    "    for ncbi_id, original_seq in original_full_protein_sequences_dict.items():\n",
    "        # Find the maximum length of any peptide associated with this protein\n",
    "        peptides_for_this_protein = valid_peptides_df[valid_peptides_df['NCBI_id'] == ncbi_id]\n",
    "        if not peptides_for_this_protein.empty:\n",
    "            max_peptide_len = max(len(p_seq) for p_seq in peptides_for_this_protein['Aminoacids'] if isinstance(p_seq, str))\n",
    "        else:\n",
    "            max_peptide_len = 0 \n",
    "\n",
    "        # The protein needs to be at least as long as its original sequence, AND as long as the longest peptide\n",
    "        target_len_for_extension = max(len(original_seq), max_peptide_len) \n",
    "        \n",
    "        extended_seq = _extend_protein_sequence(original_seq, target_len_for_extension)\n",
    "        extended_full_protein_sequences_dict[ncbi_id] = extended_seq\n",
    "        \n",
    "        extended_protein_nsp3_input_df_data.append({\n",
    "            'identifier': ncbi_id,\n",
    "            'Aminoacids': extended_seq\n",
    "        })\n",
    "        \n",
    "        print(f\"[DEBUG] Protein {ncbi_id}: Original length {len(original_seq)}, Max peptide length for associated peptides {max_peptide_len}, Extended to {len(extended_seq)}.\")\n",
    "\n",
    "    extended_protein_nsp3_input_df = pd.DataFrame(extended_protein_nsp3_input_df_data)\n",
    "    # --- END NEW LOGIC ---\n",
    "\n",
    "    # 2. Run NetSurfP-3 for RSA on all unique *extended* relevant full proteins (PARALLELIZED)\n",
    "    protein_rsa_map = run_netsurfp3_standalone_prediction(extended_protein_nsp3_input_df, num_netsurfp3_processes=num_netsurfp3_processes)\n",
    "    print(\"[INFO] NetSurfP-3 RSA prediction complete for relevant proteins (using extended sequences).\")\n",
    "    \n",
    "    # 3. Process each peptide, now using the extended sequences and their RSA scores\n",
    "    results = []\n",
    "    print(f\"\\n[INFO] Calculating Buried/Exposed for {len(valid_peptides_df)} target peptides...\")\n",
    "    for _, row in valid_peptides_df.iterrows():\n",
    "        peptide_id = row['identifier']\n",
    "        peptide_seq = row['Aminoacids']\n",
    "        ncbi_id = row['NCBI_id']\n",
    "        \n",
    "        # Retrieve the *extended* protein sequence and its RSA scores\n",
    "        full_prot_seq_extended = extended_full_protein_sequences_dict.get(ncbi_id)\n",
    "        prot_rsa_scores_extended = protein_rsa_map.get(ncbi_id)\n",
    "        \n",
    "        # Call the stripped-down worker function\n",
    "        res = _process_single_peptide_buried_exposed(\n",
    "            peptide_id, peptide_seq, full_prot_seq_extended, prot_rsa_scores_extended, rsa_buried_threshold, ncbi_id\n",
    "        )\n",
    "        results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results).set_index('identifier')\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n==================================================================================\")\n",
    "    print(\"--- Running Focused Buried/Exposed Debug Script with Real Data ---\")\n",
    "    print(\"--- Implementing protein sequence extension for short proteins ---\")\n",
    "    print(\"==================================================================================\\n\")\n",
    "\n",
    "    # 1. Load your actual dataframes\n",
    "    print(f\"[INFO] Loading full_library_df from: {DATA_DIR}VP_library_all_sequences.csv\")\n",
    "    full_library_df_raw = pd.read_csv(f\"{DATA_DIR}VP_library_all_sequences.csv\")\n",
    "    \n",
    "    print(f\"[INFO] Loading full_proteins_df from: {DATA_DIR}full_library_virus_proteins.csv\")\n",
    "    full_proteins_df_raw = pd.read_csv(f\"{DATA_DIR}full_library_virus_proteins.csv\")\n",
    "    \n",
    "    # Apply initial processing steps to full_proteins_df as in your original script\n",
    "    full_proteins_df_raw['NCBI_id'] = full_proteins_df_raw['NCBI_id'].str.split('|').str[0]\n",
    "    if 'Protein Sequence' in full_proteins_df_raw.columns:\n",
    "        full_proteins_df_raw.rename(columns={'Protein Sequence': 'Sequence'}, inplace=True)\n",
    "    elif 'sequence' in full_proteins_df_raw.columns:\n",
    "        full_proteins_df_raw.rename(columns={'sequence': 'Sequence'}, inplace=True)\n",
    "    \n",
    "    print(f\"[INFO] Initial full_library_df_raw shape: {full_library_df_raw.shape}\")\n",
    "    print(f\"[INFO] Initial full_proteins_df_raw shape: {full_proteins_df_raw.shape}\")\n",
    "\n",
    "    # 2. Filter full_library_df for VT/VP peptides and then for the target problematic IDs\n",
    "    full_library_filtered_vt_vp = full_library_df_raw[full_library_df_raw['code'].isin(['VT', 'VP'])].copy()\n",
    "    \n",
    "    target_peptides_df = full_library_filtered_vt_vp[\n",
    "        full_library_filtered_vt_vp['identifier'].isin(TARGET_PEPTIDE_IDS)\n",
    "    ].copy()\n",
    "    \n",
    "    if target_peptides_df.empty:\n",
    "        print(f\"\\n[ERROR] No peptides found matching the TARGET_PEPTIDE_IDS: {TARGET_PEPTIDE_IDS}. Please check IDs.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"\\n[INFO] Found {len(target_peptides_df)} target peptides for analysis.\")\n",
    "    print(\"Target Peptides Data:\\n\", target_peptides_df[['identifier', 'Aminoacids', 'NCBI_id']])\n",
    "\n",
    "    # 3. Call the adapted calculation function with the new extension logic\n",
    "    final_results_df = calculate_filtered_peptide_properties(\n",
    "        peptides_df=target_peptides_df[['identifier', 'Aminoacids', 'NCBI_id']],\n",
    "        full_proteins_df=full_proteins_df_raw,\n",
    "        rsa_buried_threshold=RSA_BURIED_THRESHOLD,\n",
    "        num_netsurfp3_processes=min(os.cpu_count() or 1, 2)\n",
    "    )\n",
    "\n",
    "    print(\"\\n==================================================================================\")\n",
    "    print(\"--- Final Buried/Exposed Results for Target Peptides ---\")\n",
    "    print(\"==================================================================================\")\n",
    "    print(final_results_df)\n",
    "    \n",
    "    print(\"\\n[INFO] Script finished. Examine the debug output above for details on each peptide.\")\n",
    "    print(\"If 'Buried_perc' and 'Exposed_perc' are 0.00% for a peptide, look for 'Warning:' messages above.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
