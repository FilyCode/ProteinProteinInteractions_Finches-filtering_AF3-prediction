{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess # Needed to run external commands\n",
    "import tempfile   # Needed to create temporary files\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import metapredict as mpp # For disorder prediction\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing  # For parallelization of metapredict calls\n",
    "import math\n",
    "\n",
    "# Define the absolute path to the cloned s4pred directory\n",
    "s4pred_path = \"/projectnb/cancergrp/Philipp/.conda/pkgs/s4pred\"\n",
    "run_model_script = os.path.join(s4pred_path, \"run_model.py\")\n",
    "\n",
    "# CONSTANTS FOR STANDALONE NETSURFP-3 \n",
    "NETSURFP3_STANDALONE_PATH = \"/projectnb/cancergrp/Philipp/NetSurfP-3.0_standalone\" \n",
    "\n",
    "# The name of the conda environment for the standalone NetSurfP-3 (as created from its environment.yml)\n",
    "NETSURFP3_STANDALONE_ENV_NAME = \"nsp3\" \n",
    "\n",
    "# Full paths to the nsp3.py script and its model file\n",
    "NSP3_SCRIPT_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"nsp3.py\")\n",
    "NSP3_MODEL_PATH = os.path.join(NETSURFP3_STANDALONE_PATH, \"models\", \"nsp3.pth\")\n",
    "\n",
    "# Batching for parallel NetSurfP-3 calls\n",
    "NETSURFP3_BATCH_SIZE = 50 # Number of proteins to process in each parallel NetSurfP-3.0 call\n",
    "\n",
    "# Netsurfp3 parameter\n",
    "RSA_BURIED_THRESHOLD = 0.25 # Relative Solvent Accessibility threshold: <=0.25 is buried, >0.25 is exposed\n",
    "\n",
    "\n",
    "DATA_DIR = \"/projectnb/cancergrp/Philipp/data/\"\n",
    "RESULTS_DIR = \"/projectnb/cancergrp/Philipp/results/RITA_peptides\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "full_library_df = pd.read_csv(f\"{DATA_DIR}VP_library_all_sequences.csv\")\n",
    "RITA_exp_df = pd.read_excel(f\"{DATA_DIR}RITA_and_ABT_pos_selection_screens.xlsx\", sheet_name='RITA')\n",
    "\n",
    "# Load the full proteins DataFrame\n",
    "full_proteins_df = pd.read_csv(f\"{DATA_DIR}full_library_virus_proteins.csv\")\n",
    "full_proteins_df['NCBI_id'] = full_proteins_df['NCBI_id'].str.split('|').str[0]\n",
    "# Ensure the protein sequence column is named 'Sequence' for consistency\n",
    "if 'Protein Sequence' in full_proteins_df.columns:\n",
    "    full_proteins_df.rename(columns={'Protein Sequence': 'Sequence'}, inplace=True)\n",
    "elif 'sequence' in full_proteins_df.columns: # Check for lowercase 'sequence' too\n",
    "    full_proteins_df.rename(columns={'sequence': 'Sequence'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only used to test smaller subsets for debuggin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PEPTIDES_FOR_TEST = 200\n",
    "\n",
    "# 1. Select a subset of peptides from the already filtered library\n",
    "test_full_library_filtered = full_library_df.head(N_PEPTIDES_FOR_TEST).copy()\n",
    "\n",
    "# 2. Get the unique NCBI_ids for these test peptides\n",
    "test_ncbi_ids = test_full_library_filtered['NCBI_id'].unique()\n",
    "\n",
    "# 3. Filter the full proteins DataFrame to include only proteins matching these NCBI_ids\n",
    "test_full_proteins_df = full_proteins_df[full_proteins_df['NCBI_id'].isin(test_ncbi_ids)].copy()\n",
    "\n",
    "# 4. Get the unique identifiers (tileID) for these test peptides\n",
    "#    RITA_exp_filtered already has 'identifier' column from 'tileID'\n",
    "test_peptide_identifiers = test_full_library_filtered['identifier'].unique()\n",
    "\n",
    "# 5. Filter the RITA experimental data to include only entries matching these peptide identifiers\n",
    "test_RITA_exp_filtered = RITA_exp_df[RITA_exp_df['tileID'].isin(test_peptide_identifiers)].copy()\n",
    "\n",
    "# --- Replace the original DataFrames with these test subsets ---\n",
    "full_library_df = test_full_library_filtered\n",
    "RITA_exp_df = test_RITA_exp_filtered\n",
    "full_proteins_df = test_full_proteins_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Aminoacid distributions for peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 20 standard amino acids for consistent ordering in plots\n",
    "AMINO_ACIDS = sorted(list('ACDEFGHIKLMNPQRSTVWY'))\n",
    "\n",
    "# Helper Function to Calculate Amino Acid Composition \n",
    "def get_amino_acid_composition(sequences):\n",
    "    \"\"\"\n",
    "    Calculates the amino acid composition (percentage) for a list of peptide sequences.\n",
    "    Handles empty sequences or non-string entries gracefully.\n",
    "    \"\"\"\n",
    "    # CORRECTED: Use .empty to check if the pandas Series is empty\n",
    "    if sequences.empty:\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    total_aa_counts = Counter()\n",
    "    total_length = 0\n",
    "    # Filter out non-strings or empty strings before processing\n",
    "    valid_sequences = [s for s in sequences if isinstance(s, str) and s]\n",
    "\n",
    "    # If after filtering, there are no valid sequences, return zeros\n",
    "    if not valid_sequences:\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    for seq in valid_sequences:\n",
    "        total_aa_counts.update(seq)\n",
    "        total_length += len(seq)\n",
    "\n",
    "    if total_length == 0: # This handles cases where valid_sequences might contain only empty strings\n",
    "        return pd.Series({aa: 0.0 for aa in AMINO_ACIDS}, name=\"Composition\")\n",
    "\n",
    "    composition = {aa: (total_aa_counts.get(aa, 0) / total_length) * 100 for aa in AMINO_ACIDS}\n",
    "    return pd.Series(composition, name=\"Composition\")\n",
    "\n",
    "# Helper Function to Plot Amino Acid Composition \n",
    "def plot_composition(composition_series_dict, title, filename_prefix, results_dir):\n",
    "    \"\"\"\n",
    "    Plots amino acid composition for one or more groups using grouped bar plots.\n",
    "    composition_series_dict: dict of {group_name: pandas.Series of composition}\n",
    "    \"\"\"\n",
    "    if not composition_series_dict:\n",
    "        print(f\"Skipping plot '{title}': No data provided.\")\n",
    "        return\n",
    "\n",
    "    # Convert dictionary of Series to a DataFrame for easier plotting\n",
    "    plot_df_data = []\n",
    "    for group_name, series in composition_series_dict.items():\n",
    "        if series is not None and not series.empty: # Ensure series is not None or empty\n",
    "            temp_df = series.reset_index()\n",
    "            temp_df.columns = ['Amino Acid', 'Percentage']\n",
    "            temp_df['Group'] = group_name\n",
    "            plot_df_data.append(temp_df)\n",
    "        else:\n",
    "            print(f\"Warning: No valid composition data for group '{group_name}' in '{title}'.\")\n",
    "\n",
    "    if not plot_df_data:\n",
    "        print(f\"Skipping plot '{title}': No valid dataframes to concatenate.\")\n",
    "        return\n",
    "\n",
    "    plot_df = pd.concat(plot_df_data)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    sns.barplot(data=plot_df, x='Amino Acid', y='Percentage', hue='Group', palette='viridis', ci=None) # ci=None for no confidence intervals as it's aggregated data\n",
    "    plt.title(f'Amino Acid Composition: {title}', fontsize=16)\n",
    "    plt.xlabel('Amino Acid', fontsize=12)\n",
    "    plt.ylabel('Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_dir, f\"{filename_prefix}_amino_acid_composition.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "# Filter DataFrames for VT or VP peptides \n",
    "print(\"\\nFiltering Data \")\n",
    "full_library_filtered = full_library_df[full_library_df['code'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered = RITA_exp_df[RITA_exp_df['type'].isin(['VT', 'VP'])].copy()\n",
    "\n",
    "print(f\"Number of peptides in full library (VT/VP): {len(full_library_filtered)}\")\n",
    "print(f\"Number of peptides used in RITA experiment (VT/VP): {len(RITA_exp_filtered)}\")\n",
    "\n",
    "\n",
    "print(\"\\nGenerating comprehensive peptide amino acid composition and metadata table...\")\n",
    "\n",
    "# Start with the basic peptide info (identifier and sequence) from the filtered full library\n",
    "comprehensive_peptide_table_aa = full_library_filtered[['identifier', 'Aminoacids']].copy()\n",
    "\n",
    "# Calculate amino acid percentages for each individual peptide\n",
    "aa_composition_per_peptide_df = comprehensive_peptide_table_aa['Aminoacids'].apply(lambda seq: pd.Series({\n",
    "    aa: (Counter(seq).get(aa, 0) / len(seq)) * 100 if len(seq) > 0 else 0.0\n",
    "    for aa in AMINO_ACIDS\n",
    "}))\n",
    "\n",
    "# Concatenate the calculated percentages with the initial identifier and Aminoacids columns\n",
    "comprehensive_peptide_table_aa = pd.concat([comprehensive_peptide_table_aa, aa_composition_per_peptide_df], axis=1)\n",
    "\n",
    "# Prepare RITA experiment data for merging\n",
    "RITA_exp_filtered['identifier'] = RITA_exp_filtered['tileID']\n",
    "rita_metadata_for_merge_aa = RITA_exp_filtered[['identifier', 'sig', 'log2FoldChange', 'padj']].copy()\n",
    "\n",
    "# Ensure log2FoldChange and padj are numeric before potential calculations or display\n",
    "rita_metadata_for_merge_aa['log2FoldChange'] = pd.to_numeric(rita_metadata_for_merge_aa['log2FoldChange'], errors='coerce')\n",
    "rita_metadata_for_merge_aa['padj'] = pd.to_numeric(rita_metadata_for_merge_aa['padj'], errors='coerce')\n",
    "\n",
    "\n",
    "# Merge RITA experiment metadata\n",
    "# Use a left merge to ensure all peptides from comprehensive_peptide_table_aa are kept.\n",
    "# Peptides not found in rita_metadata_for_merge_aa (i.e., not used in the experiment)\n",
    "# will have NaN values in the newly merged 'significant', 'log_FC', and 'adj_p_val' columns.\n",
    "comprehensive_peptide_table_aa = comprehensive_peptide_table_aa.merge(\n",
    "    rita_metadata_for_merge_aa,\n",
    "    on='identifier',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the new table\n",
    "print(\"\\nFirst 5 rows of the comprehensive peptide amino acid composition table:\")\n",
    "print(comprehensive_peptide_table_aa.head())\n",
    "print(f\"\\nShape of the comprehensive peptide amino acid composition table: {comprehensive_peptide_table_aa.shape}\")\n",
    "print(f\"Columns in the comprehensive peptide amino acid composition table: {comprehensive_peptide_table_aa.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Save the comprehensive table to a CSV file\n",
    "comprehensive_table_path_aa = os.path.join(RESULTS_DIR, \"comprehensive_peptide_amino_acid_composition_and_metadata.csv\")\n",
    "comprehensive_peptide_table_aa.to_csv(comprehensive_table_path_aa, index=False)\n",
    "print(f\"\\nSaved comprehensive peptide amino acid composition table to: {comprehensive_table_path_aa}\")\n",
    "\n",
    "\n",
    "# Calculate Amino Acid Compositions for Each Group \n",
    "print(\"\\nCalculating Amino Acid Compositions \")\n",
    "\n",
    "# Full Library (VT/VP only)\n",
    "comp_full_library = get_amino_acid_composition(full_library_filtered['Aminoacids'])\n",
    "\n",
    "# Experiment Used (VT/VP only)\n",
    "comp_exp_used = get_amino_acid_composition(RITA_exp_filtered['Aminoacids'])\n",
    "\n",
    "# Peptides Used vs. Not Used in Experiment (from the VT/VP filtered library)\n",
    "used_sequences_set = set(RITA_exp_filtered['Aminoacids'].unique())\n",
    "not_used_peptides_df = full_library_filtered[~full_library_filtered['Aminoacids'].isin(used_sequences_set)]\n",
    "\n",
    "comp_exp_not_used = get_amino_acid_composition(not_used_peptides_df['Aminoacids'])\n",
    "\n",
    "num_used = len(used_sequences_set)\n",
    "num_not_used = len(not_used_peptides_df['Aminoacids'].unique()) # Unique counts for 'not used'\n",
    "print(f\"\\nComparison of Used vs. Not Used peptides (from VT/VP library):\")\n",
    "print(f\"  Total unique peptides in full library (VT/VP): {len(full_library_filtered['Aminoacids'].unique())}\")\n",
    "print(f\"  Unique peptides USED in experiment: {num_used} ({num_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "print(f\"  Unique peptides NOT USED in experiment: {num_not_used} ({num_not_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "\n",
    "\n",
    "# Experiment Significant vs. Non-Significant (from VT/VP used in experiment)\n",
    "RITA_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'Yes']\n",
    "RITA_non_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'No']\n",
    "\n",
    "comp_exp_sig = get_amino_acid_composition(RITA_sig['Aminoacids'])\n",
    "comp_exp_non_sig = get_amino_acid_composition(RITA_non_sig['Aminoacids'])\n",
    "\n",
    "print(f\"\\nSignificant vs. Non-Significant peptides (from VT/VP used in experiment):\")\n",
    "print(f\"  Number of significant peptides: {len(RITA_sig)}\")\n",
    "print(f\"  Number of non-significant peptides: {len(RITA_non_sig)}\")\n",
    "\n",
    "# Experiment Upregulated vs. Downregulated Significant (from VT/VP used and significant)\n",
    "# Ensure log2FoldChange is numeric before comparison\n",
    "RITA_sig['log2FoldChange'] = pd.to_numeric(RITA_sig['log2FoldChange'], errors='coerce')\n",
    "RITA_up = RITA_sig[RITA_sig['log2FoldChange'] > 0]\n",
    "RITA_down = RITA_sig[RITA_sig['log2FoldChange'] < 0]\n",
    "\n",
    "comp_exp_up = get_amino_acid_composition(RITA_up['Aminoacids'])\n",
    "comp_exp_down = get_amino_acid_composition(RITA_down['Aminoacids'])\n",
    "\n",
    "print(f\"\\nUpregulated vs. Downregulated Significant peptides:\")\n",
    "print(f\"  Number of upregulated significant peptides: {len(RITA_up)}\")\n",
    "print(f\"  Number of downregulated significant peptides: {len(RITA_down)}\")\n",
    "print(f\"  Number of significant peptides with logFC = 0 (or NaN): {len(RITA_sig) - len(RITA_up) - len(RITA_down)}\")\n",
    "\n",
    "\n",
    "# Combine Compositions into a Summary DataFrame and Save \n",
    "print(\"\\nGenerating Summary Table \")\n",
    "all_compositions = pd.DataFrame({\n",
    "    'Full_Library_VT_VP': comp_full_library,\n",
    "    'Experiment_Used_VT_VP': comp_exp_used,\n",
    "    'Experiment_Not_Used_VT_VP': comp_exp_not_used,\n",
    "    'Experiment_Significant_VT_VP': comp_exp_sig,\n",
    "    'Experiment_NonSignificant_VT_VP': comp_exp_non_sig,\n",
    "    'Experiment_Upregulated_VT_VP': comp_exp_up,\n",
    "    'Experiment_Downregulated_VT_VP': comp_exp_down\n",
    "})\n",
    "\n",
    "# Round to 2 decimal places for presentation\n",
    "all_compositions = all_compositions.round(2)\n",
    "\n",
    "print(\"\\nAmino Acid Composition Summary (Percentages):\")\n",
    "print(all_compositions)\n",
    "\n",
    "summary_table_path = os.path.join(RESULTS_DIR, \"amino_acid_composition_summary.csv\")\n",
    "all_compositions.to_csv(summary_table_path)\n",
    "print(f\"\\nSaved amino acid composition summary table: {summary_table_path}\")\n",
    "\n",
    "# Plotting the Compositions \n",
    "print(\"\\nGenerating Plots \")\n",
    "\n",
    "# Plot 1: Full Library vs. Experiment Used\n",
    "plot_composition(\n",
    "    {'Full Library (VT/VP)': comp_full_library, 'Experiment Used (VT/VP)': comp_exp_used},\n",
    "    'Full Library vs. Experiment Used Peptides (VT/VP)',\n",
    "    'full_vs_used',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 2: Experiment Used vs. Not Used\n",
    "plot_composition(\n",
    "    {'Experiment Used (VT/VP)': comp_exp_used, 'Experiment Not Used (VT/VP)': comp_exp_not_used},\n",
    "    'Experiment Used vs. Not Used Peptides (VT/VP)',\n",
    "    'used_vs_not_used',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 3: Experiment Significant vs. Non-Significant\n",
    "plot_composition(\n",
    "    {'Significant (VT/VP)': comp_exp_sig, 'Non-Significant (VT/VP)': comp_exp_non_sig},\n",
    "    'Experiment Significant vs. Non-Significant Peptides (VT/VP)',\n",
    "    'significant_vs_nonsignificant',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "# Plot 4: Experiment Upregulated vs. Downregulated Significant\n",
    "plot_composition(\n",
    "    {'Upregulated Significant (VT/VP)': comp_exp_up, 'Downregulated Significant (VT/VP)': comp_exp_down},\n",
    "    'Upregulated vs. Downregulated Significant Peptides (VT/VP)',\n",
    "    'upregulated_vs_downregulated',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "print(\"\\nAnalysis Complete! Check your results directory for the summary CSV and plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting structural properties from peptides and information about its location in protein (buried or exposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_full_protein_info(full_library_proteins_df):\n",
    "    \"\"\"\n",
    "    Extracts unique full protein sequences and their NCBI_ids for NetSurfP-3 input.\n",
    "    Returns a dictionary {NCBI_id: sequence} and a DataFrame suitable for NetsurfP-3 input.\n",
    "    Assumes full_library_proteins_df has 'NCBI_id' and a 'Sequence' column.\n",
    "    \"\"\"\n",
    "    # Ensure 'Sequence' column is string and not empty, and 'NCBI_id' is present\n",
    "    unique_proteins_df = full_library_proteins_df[\n",
    "        full_library_proteins_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "    \n",
    "    # Drop duplicates based on NCBI_id and sequence to ensure unique proteins for prediction\n",
    "    unique_proteins_df = unique_proteins_df.drop_duplicates(subset=['NCBI_id', 'Sequence'])\n",
    "\n",
    "    protein_sequences_dict = unique_proteins_df.set_index('NCBI_id')['Sequence'].to_dict()\n",
    "    # Create a DataFrame for input to NetSurfP-3\n",
    "    protein_input_df = unique_proteins_df[['NCBI_id', 'Sequence']].rename(columns={'NCBI_id': 'identifier', 'Sequence': 'Aminoacids'})\n",
    "    return protein_sequences_dict, protein_input_df\n",
    "\n",
    "# Helper function for buried/exposed prediction with netsurfp3\n",
    "def _run_single_netsurfp3_batch(batch_input_df_args):\n",
    "    \"\"\"\n",
    "    Worker function to run standalone NetSurfP-3 for a single batch of proteins.\n",
    "    Returns a dictionary of {protein_id: [RSA_scores]} for the batch.\n",
    "    \"\"\"\n",
    "    batch_index, batch_df = batch_input_df_args\n",
    "    batch_results = {}\n",
    "    fasta_input_path = None\n",
    "    output_dir = None\n",
    "\n",
    "    KEEP_TEMP_FILES = False \n",
    "\n",
    "    try:\n",
    "        # Check if batch_df is empty after potential subsetting or filtering\n",
    "        if batch_df.empty:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): Input DataFrame for NetSurfP-3 batch is empty. Skipping.\\n\")\n",
    "            return batch_results\n",
    "\n",
    "        # Create temporary FASTA input file for this batch\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=f\"_batch{batch_index}.fasta\") as fasta_file:\n",
    "            fasta_input_path = fasta_file.name\n",
    "            for _, row in batch_df.iterrows():\n",
    "                fasta_file.write(f\">{row['identifier']}\\n{row['Aminoacids']}\\n\")\n",
    "\n",
    "        # Create a temporary output directory for this batch's NetSurfP-3.0 results\n",
    "        output_dir = tempfile.mkdtemp(prefix=f\"nsp3_standalone_output_batch{batch_index}_\")\n",
    "\n",
    "        # Prepare and run the standalone NetSurfP-3.0 script using 'conda run'\n",
    "        command = [\n",
    "            \"conda\", \"run\", \"-n\", NETSURFP3_STANDALONE_ENV_NAME,\n",
    "            \"python\", NSP3_SCRIPT_PATH,\n",
    "            \"-m\", NSP3_MODEL_PATH,\n",
    "            \"-i\", fasta_input_path,\n",
    "            \"-o\", output_dir # Output directory, as per standalone README\n",
    "        ]\n",
    "        \n",
    "        print(f\" (Batch {batch_index}): Running NetSurfP-3 command: {' '.join(command)}\")\n",
    "\n",
    "        # Execute the command, capturing stdout and stderr.\n",
    "        process = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "               \n",
    "        print(f\"DEBUG (Batch {batch_index}): NetSurfP-3 standalone prediction command executed. Checking output directory...\")\n",
    "\n",
    "        # Find the subdirectory created by NetSurfP-3.0 (e.g., '01', '02', etc.)\n",
    "        # This subdirectory contains the actual output files for the batch.\n",
    "        subdirs = [d for d in os.listdir(output_dir) if os.path.isdir(os.path.join(output_dir, d))]\n",
    "        \n",
    "        if not subdirs:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): No subdirectories found in {output_dir}. NetSurfP-3 output structure may have changed.\\n\")\n",
    "            return batch_results # Return empty if no subdirs\n",
    "        \n",
    "        # Assume there's only one relevant subdirectory per batch for now (e.g., '01', '02', etc. per job run)\n",
    "        job_output_subdir_name = subdirs[0] # e.g., '01' based on your debug output\n",
    "        job_output_subdir_path = os.path.join(output_dir, job_output_subdir_name)\n",
    "\n",
    "        # Construct the path to the main CSV output file within that subdirectory.\n",
    "        # Based on your `head 01.csv` output, the file is named after the subdir (e.g., '01.csv').\n",
    "        main_csv_filepath = os.path.join(job_output_subdir_path, f\"{job_output_subdir_name}.csv\") \n",
    "\n",
    "        if not os.path.exists(main_csv_filepath):\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): Expected CSV file '{main_csv_filepath}' not found.\\n\")\n",
    "            sys.stderr.write(f\"Files in {job_output_subdir_path}: {os.listdir(job_output_subdir_path)}\\n\")\n",
    "            return batch_results\n",
    "\n",
    "        protein_rsa_scores_accumulator = {} # Accumulate scores for each protein in this batch\n",
    "        with open(main_csv_filepath, 'r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "\n",
    "            # --- CRITICAL FIX: Look for ' rsa' with a leading space and 'id' as key ---\n",
    "            if ' rsa' not in reader.fieldnames: # (yes it needs to have a space, as the developer put spaces in the columns)\n",
    "                sys.stderr.write(f\"Warning (Batch {batch_index}): ' rsa' column not found in {main_csv_filepath}. Found: {reader.fieldnames}. Skipping this file.\\n\")\n",
    "                return batch_results\n",
    "\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    # The 'id' column contains '>YP_009944365.1'. Strip the '>' prefix.\n",
    "                    current_protein_id = row['id'].lstrip('>') \n",
    "                    rsa_value = float(row[' rsa']) # Get RSA value from lowercase ' rsa' column (yes it needs to have a space, as the developer put spaces in the columns)\n",
    "                    \n",
    "                    if current_protein_id not in protein_rsa_scores_accumulator:\n",
    "                        protein_rsa_scores_accumulator[current_protein_id] = []\n",
    "                    \n",
    "                    protein_rsa_scores_accumulator[current_protein_id].append(rsa_value)\n",
    "                except (ValueError, KeyError) as parse_e:\n",
    "                    sys.stderr.write(f\"Warning (Batch {batch_index}): Error parsing row in {main_csv_filepath}: {row}. Error: {parse_e}. Skipping row.\\n\")\n",
    "                    continue\n",
    "        \n",
    "        # After processing all rows, populate batch_results\n",
    "        batch_results.update(protein_rsa_scores_accumulator)\n",
    "        \n",
    "        if not batch_results:\n",
    "            sys.stderr.write(f\"Warning (Batch {batch_index}): No valid RSA scores were extracted from {main_csv_filepath}.\\n\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        sys.stderr.write(f\"Error running standalone NetSurfP-3 for batch {batch_index} (command: {' '.join(command)}): {e}\\n\")\n",
    "        sys.stderr.write(f\"Stdout (first 500 chars):\\n{e.stdout[:500]}...\\n\")\n",
    "        sys.stderr.write(f\"Stderr:\\n{e.stderr}\\n\")\n",
    "        raise # Re-raise the error as it's critical\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        sys.stderr.write(f\"Error for batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f\"An unexpected error occurred during standalone NetSurfP-3 execution or output parsing for batch {batch_index}: {e}\\n\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up temporary files and directories for this batch\n",
    "        if not KEEP_TEMP_FILES: # Only delete if KEEP_TEMP_FILES is False\n",
    "            if fasta_input_path and os.path.exists(fasta_input_path):\n",
    "                os.remove(fasta_input_path)\n",
    "            if output_dir and os.path.exists(output_dir):\n",
    "                import shutil\n",
    "                shutil.rmtree(output_dir)\n",
    "            \n",
    "    return batch_results\n",
    "\n",
    "\n",
    "def run_netsurfp3_standalone_prediction(input_df, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Runs NetSurfP-3 prediction for a given DataFrame of proteins using the standalone package,\n",
    "    parallelizing by splitting into batches.\n",
    "    Args:\n",
    "        input_df (pd.DataFrame): DataFrame with 'identifier' and 'Aminoacids' columns (for FASTA input).\n",
    "        num_netsurfp3_processes (int, optional): Number of parallel processes to use for NetSurfP-3.\n",
    "                                                If None, uses min(os.cpu_count() or 1, 4).\n",
    "    Returns:\n",
    "        dict: Parsed prediction results: {identifier: [RSA_scores_list]}.\n",
    "              Returns an empty dict if input_df is empty.\n",
    "    Raises:\n",
    "        Exception: If any error occurs during standalone execution or parsing.\n",
    "    \"\"\"\n",
    "    if input_df.empty:\n",
    "        print(\"Warning: No proteins to predict for NetSurfP-3 standalone.\")\n",
    "        return {}\n",
    "\n",
    "    if not os.path.exists(NSP3_SCRIPT_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone script not found: {NSP3_SCRIPT_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH.\")\n",
    "    if not os.path.exists(NSP3_MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"NetSurfP-3 standalone model not found: {NSP3_MODEL_PATH}. \"\n",
    "                                f\"Please verify NETSURFP3_STANDALONE_PATH and 'models/nsp3.pth'.\")\n",
    "    \n",
    "    if num_netsurfp3_processes is None:\n",
    "        num_netsurfp3_processes = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "    # Split the input DataFrame into batches\n",
    "    num_proteins = len(input_df)\n",
    "    num_batches = math.ceil(num_proteins / NETSURFP3_BATCH_SIZE)\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * NETSURFP3_BATCH_SIZE\n",
    "        end_idx = min((i + 1) * NETSURFP3_BATCH_SIZE, num_proteins)\n",
    "        batches.append((i, input_df.iloc[start_idx:end_idx])) # Pass index and batch_df\n",
    "\n",
    "    print(f\"\\nRunning {num_proteins} proteins in {num_batches} batches on {num_netsurfp3_processes} processes for NetSurfP-3 (standalone)...\")\n",
    "    \n",
    "    total_rsa_map = {}\n",
    "    with multiprocessing.Pool(processes=num_netsurfp3_processes) as pool:\n",
    "        # Use imap_unordered for tqdm progress bar and yield results as they complete\n",
    "        for batch_results in tqdm(pool.imap_unordered(_run_single_netsurfp3_batch, batches),\n",
    "                                  total=num_batches,\n",
    "                                  desc=f\"NetSurfP-3 batches ({num_netsurfp3_processes} cores)\"):\n",
    "            total_rsa_map.update(batch_results) # Aggregate results from each batch\n",
    "            print(batch_results)\n",
    "\n",
    "    print(input_df)\n",
    "\n",
    "    return total_rsa_map\n",
    "\n",
    "\n",
    "# WORKER FUNCTION FOR PARALLEL PROCESSING\n",
    "def _process_single_peptide_properties(args):\n",
    "    \"\"\"\n",
    "    Worker function to calculate properties for a single peptide, including buried/exposed status.\n",
    "    This function will be run in parallel processes.\n",
    "    \"\"\"\n",
    "    peptide_id, seq, s4pred_pred_string, threshold_disorder, full_protein_seq, protein_rsa_scores, rsa_buried_threshold = args\n",
    "\n",
    "    initial_props = {f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES}\n",
    "    return_dict = {'identifier': peptide_id, **initial_props}\n",
    "\n",
    "    if len(seq) == 0:\n",
    "        return return_dict\n",
    "\n",
    "    total_residues = len(seq)\n",
    "\n",
    "    # 1. Disorder Prediction (metapredict)\n",
    "    disorder_scores = mpp.predict_disorder(seq)\n",
    "\n",
    "    # Secondary structure prediction string is passed in\n",
    "    if len(s4pred_pred_string) != len(seq):\n",
    "        print(f\"Warning (process {os.getpid()}): s4pred prediction length mismatch for peptide {peptide_id} ({len(s4pred_pred_string)} vs {len(seq)}). Assuming all coil for SS part.\")\n",
    "        s4pred_pred_string = 'C' * len(seq)\n",
    "\n",
    "    # 2. Combine and make mutually exclusive: Disorder (D) overrides SS (H, E, C)\n",
    "    combined_prediction = []\n",
    "    for j in range(len(seq)):\n",
    "        if disorder_scores[j] >= threshold_disorder:\n",
    "            combined_prediction.append('D') # Disordered\n",
    "        else:\n",
    "            combined_prediction.append(s4pred_pred_string[j]) # H, E, or C\n",
    "    combined_prediction_string = \"\".join(combined_prediction)\n",
    "\n",
    "    # 3. Calculate SS and Disorder percentages\n",
    "    return_dict['Disorder_perc'] = (combined_prediction_string.count('D') / total_residues) * 100\n",
    "    return_dict['Helix_perc'] = (combined_prediction_string.count('H') / total_residues) * 100\n",
    "    return_dict['Sheet_perc'] = (combined_prediction_string.count('E') / total_residues) * 100\n",
    "    return_dict['Coil_perc'] = (combined_prediction_string.count('C') / total_residues) * 100\n",
    "\n",
    "    # 4. Buried/Exposed calculation using full protein RSA scores\n",
    "    if full_protein_seq and protein_rsa_scores and len(full_protein_seq) == len(protein_rsa_scores):\n",
    "        # Remove leading M (artifact from sequencing) to match to original protein\n",
    "        search_seq = seq\n",
    "        stripped_m = False\n",
    "        if seq.startswith('M') and len(seq) > 1:\n",
    "            # If the peptide starts with 'M' and is long enough to strip,\n",
    "            # use the M-stripped version for finding in the full protein.\n",
    "            search_seq = seq[1:]\n",
    "            stripped_m = True\n",
    "            \n",
    "        start_index = full_protein_seq.find(search_seq)        \n",
    "\n",
    "        if start_index != -1:\n",
    "            # Adjust slice length if 'M' was stripped, the RSA scores correspond to the full_protein_seq\n",
    "            # so the slice should still be the length of the *original* peptide if we expect a 1:1 match.\n",
    "            # However, if the M was spurious, we want the RSA for the *matched* part.\n",
    "            # The RSA scores for the peptide should match the length of `search_seq`.\n",
    "            # If the M was stripped, the peptide (seq) is 1 longer than search_seq.\n",
    "            \n",
    "            # This logic assumes protein_rsa_scores align with full_protein_seq, and we want\n",
    "            # RSA for the *part of the protein that matches our peptide's core sequence*.\n",
    "            peptide_rsa_slice = protein_rsa_scores[start_index : start_index + len(search_seq)]\n",
    "            \n",
    "            if len(peptide_rsa_slice) == len(search_seq):\n",
    "                buried_count = sum(1 for rsa in peptide_rsa_slice if rsa < rsa_buried_threshold)\n",
    "                exposed_count = len(peptide_rsa_slice) - buried_count\n",
    "                \n",
    "                return_dict['Buried_perc'] = (buried_count / len(search_seq)) * 100 # Calculate percentage over search_seq length\n",
    "                return_dict['Exposed_perc'] = (exposed_count / len(search_seq)) * 100\n",
    "            else:\n",
    "                print(f\"Warning: RSA slice length mismatch for peptide '{seq[:20]}...' (ID: {peptide_id}, stripped M: {stripped_m}). Expected {len(search_seq)}, got {len(peptide_rsa_slice)}. Buried/Exposed percentages will be 0.\")\n",
    "        else:\n",
    "            reason = \"(after stripping leading 'M')\" if stripped_m else \"(original sequence)\"\n",
    "            print(f\"Warning: Peptide '{seq[:20]}...' (ID: {peptide_id}) {reason} not found in its full protein sequence. Buried/Exposed percentages will be 0.\")\n",
    "    else:\n",
    "        print(f\"Warning: Missing full protein sequence or RSA scores for peptide {peptide_id}'s protein, or length mismatch ({len(full_protein_seq) if full_protein_seq else 0} vs {len(protein_rsa_scores) if protein_rsa_scores else 0}). Buried/Exposed percentages will be 0.\")\n",
    "\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "# Helper Function to Get Peptide Properties (Disorder, Secondary Structure, Exposed/Buried)\n",
    "def calculate_all_peptide_structural_properties(peptides_df_with_ncbi, full_proteins_df, threshold_disorder=0.5, rsa_buried_threshold=0.25, num_processes=None, num_netsurfp3_processes=None):\n",
    "    \"\"\"\n",
    "    Calculates all peptide properties (Disorder, SS, Buried/Exposed).\n",
    "    Orchestrates full protein RSA prediction and then individual peptide property calculation.\n",
    "    \n",
    "    Args:\n",
    "        peptides_df_with_ncbi (pd.DataFrame): DataFrame with 'identifier', 'Aminoacids', and 'NCBI_id' columns.\n",
    "        full_proteins_df (pd.DataFrame): DataFrame with 'NCBI_id' and 'Sequence' columns for full proteins.\n",
    "        threshold_disorder (float): Disorder score threshold for metapredict.\n",
    "        rsa_buried_threshold (float): RSA threshold for classifying residues as buried.\n",
    "        num_processes (int, optional): Number of CPU cores to use for parallel processing for general peptide properties.\n",
    "        num_netsurfp3_processes (int, optional): Number of CPU cores to use for parallel NetSurfP-3 calls.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame indexed by 'identifier' with all calculated percentage columns.\n",
    "    \"\"\"\n",
    "    if peptides_df_with_ncbi.empty:\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    valid_peptides_df = peptides_df_with_ncbi[\n",
    "        peptides_df_with_ncbi['Aminoacids'].apply(lambda x: isinstance(x, str) and len(x) > 0)\n",
    "    ].copy()\n",
    "\n",
    "    if valid_peptides_df.empty:\n",
    "        return pd.DataFrame(columns=[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES])\n",
    "\n",
    "    # 1. Get unique full proteins and their sequences for NetSurfP-3 input\n",
    "    full_protein_sequences_dict, full_protein_nsp3_input_df = get_unique_full_protein_info(full_proteins_df)\n",
    "\n",
    "    # 2. Run NetSurfP-3 for RSA on all unique full proteins using the standalone version (PARALLELIZED)\n",
    "    print(f\"\\nPredicting RSA for {len(full_protein_nsp3_input_df)} unique full proteins using NetSurfP-3 (standalone, parallelized)...\")\n",
    "    protein_rsa_map = run_netsurfp3_standalone_prediction(full_protein_nsp3_input_df, num_netsurfp3_processes=num_netsurfp3_processes)\n",
    "    print(\"NetSurfP-3 RSA prediction complete.\")\n",
    "    \n",
    "    # 3. Run S4PRED for secondary structure on all peptides (same logic as before)\n",
    "    fasta_input_path_s4pred = None\n",
    "    s4pred_output_path = None\n",
    "    s4pred_map = {}\n",
    "\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".fasta\") as fasta_file:\n",
    "            fasta_input_path_s4pred = fasta_file.name\n",
    "            for _, row in valid_peptides_df.iterrows():\n",
    "                fasta_file.write(f\">{row['identifier']}\\n{row['Aminoacids']}\\n\")\n",
    "\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".ss_fas\") as output_file:\n",
    "            s4pred_output_path = output_file.name\n",
    "            command = [sys.executable, run_model_script, \"--outfmt\", \"fas\", fasta_input_path_s4pred]\n",
    "            print(f\"\\nRunning s4pred via subprocess: {' '.join(command)}\")\n",
    "            try:\n",
    "                subprocess.run(command, check=True, stdout=output_file, stderr=subprocess.PIPE, text=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error running s4pred (command: {' '.join(command)}): {e}\")\n",
    "                print(f\"Stderr: {e.stderr}\")\n",
    "                raise\n",
    "\n",
    "        current_id = None\n",
    "        current_seq_line = None\n",
    "        with open(s4pred_output_path, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    current_id = line[1:]\n",
    "                    current_seq_line = None\n",
    "                elif current_id and current_seq_line is None:\n",
    "                    current_seq_line = line\n",
    "                elif current_id and current_seq_line is not None:\n",
    "                    s4pred_map[current_id] = line\n",
    "                    current_id = None\n",
    "                    current_seq_line = None\n",
    "\n",
    "    finally:\n",
    "        if fasta_input_path_s4pred and os.path.exists(fasta_input_path_s4pred): os.remove(fasta_input_path_s4pred)\n",
    "        if s4pred_output_path and os.path.exists(s4pred_output_path): os.remove(s4pred_output_path)\n",
    "    # END S4PRED SUBPROCESS BLOCK\n",
    "\n",
    "    # Prepare arguments for parallel processing for each peptide\n",
    "    task_args = []\n",
    "    for _, row in valid_peptides_df.iterrows():\n",
    "        peptide_id = row['identifier']\n",
    "        peptide_seq = row['Aminoacids']\n",
    "        ncbi_id = row['NCBI_id']\n",
    "        \n",
    "        full_prot_seq = full_protein_sequences_dict.get(ncbi_id)\n",
    "        prot_rsa_scores = protein_rsa_map.get(ncbi_id)\n",
    "        s4pred_ss = s4pred_map.get(peptide_id, 'C' * len(peptide_seq))\n",
    "\n",
    "        print(protein_rsa_map)\n",
    "        print(ncbi_id)\n",
    "        print(prot_rsa_scores)\n",
    "        task_args.append(\n",
    "            (peptide_id, peptide_seq, s4pred_ss, threshold_disorder, \n",
    "             full_prot_seq, prot_rsa_scores, rsa_buried_threshold)\n",
    "        )\n",
    "\n",
    "    print(f\"Calculating peptide properties for {len(valid_peptides_df)} peptides in parallel (disorder overrides SS, plus buried/exposed)...\")\n",
    "\n",
    "    if num_processes is None:\n",
    "        num_processes = min(os.cpu_count() or 1, 4)\n",
    "\n",
    "    results = []\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        for res in tqdm(pool.imap_unordered(_process_single_peptide_properties, task_args),\n",
    "                        total=len(task_args),\n",
    "                        desc=f\"Processing peptides ({num_processes} cores)\"):\n",
    "            results.append(res)\n",
    "\n",
    "    return pd.DataFrame(results).set_index('identifier')\n",
    "    \n",
    "\n",
    "# Helper Function to Plot Average Peptide Properties (no changes needed here)\n",
    "def plot_average_properties(average_properties_dict, title, filename_prefix, results_dir):\n",
    "    \"\"\"\n",
    "    Plots average peptide properties (Disorder, Helix, Sheet, Coil) for different groups.\n",
    "    average_properties_dict: dict of {group_name: pandas.Series of average properties}\n",
    "    \"\"\"\n",
    "    if not average_properties_dict:\n",
    "        print(f\"Skipping plot '{title}': No data provided.\")\n",
    "        return\n",
    "\n",
    "    plot_data = []\n",
    "    for group_name, series in average_properties_dict.items():\n",
    "        if series is not None and not series.empty:\n",
    "            # Reorder the series according to PEPTIDE_PROPERTY_TYPES for consistent plotting order\n",
    "            ordered_series = series[[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES]]\n",
    "            temp_df = ordered_series.to_frame(name='Percentage').reset_index()\n",
    "            temp_df.columns = ['Property', 'Percentage']\n",
    "            # Clean up property names for plotting (remove '_perc')\n",
    "            temp_df['Property'] = temp_df['Property'].str.replace('_perc', '')\n",
    "            temp_df['Group'] = group_name\n",
    "            plot_data.append(temp_df)\n",
    "        else:\n",
    "            print(f\"Warning: No valid average property data for group '{group_name}' in '{title}'.\")\n",
    "\n",
    "    if not plot_data:\n",
    "        print(f\"Skipping plot '{title}': No valid dataframes to concatenate.\")\n",
    "        return\n",
    "\n",
    "    plot_df = pd.concat(plot_data)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.barplot(data=plot_df, x='Property', y='Percentage', hue='Group', palette='Spectral', ci=None)\n",
    "    plt.title(f'Average Peptide Properties: {title}', fontsize=16)\n",
    "    plt.xlabel('Peptide Property', fontsize=12)\n",
    "    plt.ylabel('Average Percentage (%)', fontsize=12)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(results_dir, f\"{filename_prefix}_peptide_properties.png\")\n",
    "    plt.savefig(plot_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {plot_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Filter DataFrames for VT or VP peptides\n",
    "print(\"\\n Filtering Data\")\n",
    "full_library_filtered = full_library_df[full_library_df['code'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered = RITA_exp_df[RITA_exp_df['type'].isin(['VT', 'VP'])].copy()\n",
    "RITA_exp_filtered['identifier'] = RITA_exp_filtered['tileID']\n",
    "\n",
    "\n",
    "print(f\"Number of peptides in full library (VT/VP): {len(full_library_filtered)}\")\n",
    "print(f\"Number of peptides used in RITA experiment (VT/VP): {len(RITA_exp_filtered)}\")\n",
    "\n",
    "\n",
    "# Peptides Used vs. Not Used in Experiment (from the VT/VP filtered library)\n",
    "used_sequences_set = set(RITA_exp_filtered['Aminoacids'].unique())\n",
    "not_used_peptides_df = full_library_filtered[~full_library_filtered['Aminoacids'].isin(used_sequences_set)].copy()\n",
    "\n",
    "num_used = len(used_sequences_set)\n",
    "num_not_used = len(not_used_peptides_df['Aminoacids'].unique()) # Unique counts for 'not used'\n",
    "print(f\"\\nComparison of Used vs. Not Used peptides (from VT/VP library):\")\n",
    "print(f\"  Total unique peptides in full library (VT/VP): {len(full_library_filtered['Aminoacids'].unique())}\")\n",
    "print(f\"  Unique peptides USED in experiment: {num_used} ({num_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "print(f\"  Unique peptides NOT USED in experiment: {num_not_used} ({num_not_used / len(full_library_filtered['Aminoacids'].unique()):.2%})\")\n",
    "\n",
    "\n",
    "# Experiment Significant vs. Non-Significant (from VT/VP used in experiment)\n",
    "RITA_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'Yes'].copy()\n",
    "RITA_non_sig = RITA_exp_filtered[RITA_exp_filtered['sig'] == 'No'].copy()\n",
    "\n",
    "\n",
    "print(f\"\\nSignificant vs. Non-Significant peptides (from VT/VP used in experiment):\")\n",
    "print(f\"  Number of significant peptides: {len(RITA_sig)}\")\n",
    "print(f\"  Number of non-significant peptides: {len(RITA_non_sig)}\")\n",
    "\n",
    "# Experiment Upregulated vs. Downregulated Significant (from VT/VP used and significant)\n",
    "# Ensure log2FoldChange is numeric before comparison\n",
    "RITA_sig['log2FoldChange'] = pd.to_numeric(RITA_sig['log2FoldChange'], errors='coerce')\n",
    "RITA_up = RITA_sig[RITA_sig['log2FoldChange'] > 0].copy()\n",
    "RITA_down = RITA_sig[RITA_sig['log2FoldChange'] < 0].copy()\n",
    "\n",
    "\n",
    "print(f\"\\nUpregulated vs. Downregulated Significant peptides:\")\n",
    "print(f\"  Number of upregulated significant peptides: {len(RITA_up)}\")\n",
    "print(f\"  Number of downregulated significant peptides: {len(RITA_down)}\")\n",
    "print(f\"  Number of significant peptides with logFC = 0 (or NaN): {len(RITA_sig) - len(RITA_up) - len(RITA_down)}\")\n",
    "\n",
    "\n",
    "# Define the peptide property types for consistent ordering in plots.\n",
    "# 'Disorder' will be mutually exclusive with Helix/Sheet/Coil.\n",
    "PEPTIDE_PROPERTY_TYPES = ['Disorder', 'Helix', 'Sheet', 'Coil', 'Buried', 'Exposed']\n",
    "\n",
    "# Calculate all peptide properties once \n",
    "print(\"\\nCalculating all unique peptide properties (Disorder, Secondary Structure) ONCE\")\n",
    "all_peptides_properties_df = calculate_all_peptide_structural_properties(\n",
    "    full_library_filtered[['identifier', 'Aminoacids', 'NCBI_id']],\n",
    "    full_proteins_df,\n",
    "    threshold_disorder=0.5,\n",
    "    rsa_buried_threshold=RSA_BURIED_THRESHOLD,\n",
    "    num_processes=None\n",
    ")\n",
    "\n",
    "print(\"\\nGenerating comprehensive peptide properties and metadata table...\")\n",
    "\n",
    "# Start with the basic peptide info (identifier, sequence, NCBI_id) from the filtered full library\n",
    "comprehensive_peptide_table = full_library_filtered[['identifier', 'Aminoacids', 'NCBI_id']].copy()\n",
    "\n",
    "# Merge with the calculated structural properties from all_peptides_properties_df\n",
    "comprehensive_peptide_table = comprehensive_peptide_table.merge(\n",
    "    all_peptides_properties_df,\n",
    "    left_on='identifier',\n",
    "    right_index=True,\n",
    "    how='left' # Use left merge to keep all peptides from base table\n",
    ")\n",
    "\n",
    "# Prepare RITA experiment data for merging\n",
    "rita_metadata_for_merge = RITA_exp_filtered[['identifier', 'sig', 'log2FoldChange', 'padj']].copy()\n",
    "\n",
    "# Merge RITA experiment metadata\n",
    "comprehensive_peptide_table = comprehensive_peptide_table.merge(\n",
    "    rita_metadata_for_merge,\n",
    "    on='identifier',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the first few rows of the new table\n",
    "print(\"\\nFirst 5 rows of the comprehensive peptide table:\")\n",
    "print(comprehensive_peptide_table.head())\n",
    "print(f\"\\nShape of the comprehensive peptide table: {comprehensive_peptide_table.shape}\")\n",
    "print(f\"Columns in the comprehensive peptide table: {comprehensive_peptide_table.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Save the comprehensive table to a CSV file\n",
    "comprehensive_table_path = os.path.join(RESULTS_DIR, \"comprehensive_peptide_properties_and_metadata_with_RSA.csv\") # Updated filename\n",
    "comprehensive_peptide_table.to_csv(comprehensive_table_path, index=False)\n",
    "print(f\"\\nSaved comprehensive peptide table to: {comprehensive_table_path}\")\n",
    "\n",
    "\n",
    "#  Retrieve Peptide Properties for Each Group from the pre-calculated data \n",
    "print(\"\\nRetrieving and Averaging Peptide Properties for Each Group\")\n",
    "\n",
    "average_properties_series = {}\n",
    "\n",
    "def get_avg_props(identifiers_series, df_source):\n",
    "    if identifiers_series.empty:\n",
    "        return pd.Series({f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES})\n",
    "    unique_ids = identifiers_series.unique()\n",
    "    existing_ids = df_source.index.intersection(unique_ids)\n",
    "    if existing_ids.empty:\n",
    "        print(f\"Warning: No properties found for identifiers in group. Returning zeros for {len(unique_ids)} peptides.\")\n",
    "        return pd.Series({f'{prop}_perc': 0.0 for prop in PEPTIDE_PROPERTY_TYPES})\n",
    "    \n",
    "    props_df = df_source.loc[existing_ids]\n",
    "    # Ensure only the percentage columns are selected for mean calculation\n",
    "    return props_df[[f'{prop}_perc' for prop in PEPTIDE_PROPERTY_TYPES]].mean()\n",
    "\n",
    "\n",
    "average_properties_series['Full_Library_VT_VP'] = get_avg_props(full_library_filtered['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Used_VT_VP'] = get_avg_props(RITA_exp_filtered['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Not_Used_VT_VP'] = get_avg_props(not_used_peptides_df['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Significant_VT_VP'] = get_avg_props(RITA_sig['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_NonSignificant_VT_VP'] = get_avg_props(RITA_non_sig['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Upregulated_VT_VP'] = get_avg_props(RITA_up['identifier'], all_peptides_properties_df)\n",
    "average_properties_series['Experiment_Downregulated_VT_VP'] = get_avg_props(RITA_down['identifier'], all_peptides_properties_df)\n",
    "\n",
    "\n",
    "# Generate and Save Peptide Properties Summary Table\n",
    "print(\"\\nGenerating Peptide Properties Summary Table\")\n",
    "all_peptide_properties_avg = pd.DataFrame(average_properties_series).round(2)\n",
    "\n",
    "print(\"\\nAverage Peptide Properties Summary (Percentages):\")\n",
    "print(all_peptide_properties_avg)\n",
    "properties_summary_table_path = os.path.join(RESULTS_DIR, \"peptide_properties_summary_with_RSA.csv\") # Updated filename\n",
    "all_peptide_properties_avg.to_csv(properties_summary_table_path)\n",
    "print(f\"\\nSaved average peptide properties summary table: {properties_summary_table_path}\")\n",
    "\n",
    "\n",
    "# Plotting the Peptide Properties\n",
    "print(\"\\nGenerating Peptide Properties Plots\")\n",
    "\n",
    "plot_average_properties(\n",
    "    {'Full Library (VT/VP)': average_properties_series['Full_Library_VT_VP'],\n",
    "     'Experiment Used (VT/VP)': average_properties_series['Experiment_Used_VT_VP']},\n",
    "    'Full Library vs. Experiment Used Peptides (VT/VP)',\n",
    "    'full_vs_used_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Experiment Used (VT/VP)': average_properties_series['Experiment_Used_VT_VP'],\n",
    "     'Experiment Not Used (VT/VP)': average_properties_series['Experiment_Not_Used_VT_VP']},\n",
    "    'Experiment Used vs. Not Used Peptides (VT/VP)',\n",
    "    'used_vs_not_used_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Significant (VT/VP)': average_properties_series['Experiment_Significant_VT_VP'],\n",
    "     'Non-Significant (VT/VP)': average_properties_series['Experiment_NonSignificant_VT_VP']},\n",
    "    'Experiment Significant vs. Non-Significant Peptides (VT/VP)',\n",
    "    'significant_vs_nonsignificant_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "plot_average_properties(\n",
    "    {'Upregulated Significant (VT/VP)': average_properties_series['Experiment_Upregulated_VT_VP'],\n",
    "     'Downregulated Significant (VT/VP)': average_properties_series['Experiment_Downregulated_VT_VP']},\n",
    "    'Upregulated vs. Downregulated Significant Peptides (VT/VP)',\n",
    "    'upregulated_vs_downregulated_properties_with_RSA', # Updated filename prefix\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "plot_average_properties(\n",
    "    {'Full Library (VT/VP)': average_properties_series['Full_Library_VT_VP'],\n",
    "     'Significant (VT/VP)': average_properties_series['Experiment_Significant_VT_VP'],\n",
    "     'Non-Significant (VT/VP)': average_properties_series['Experiment_NonSignificant_VT_VP'],\n",
    "     'Upregulated Significant (VT/VP)': average_properties_series['Experiment_Upregulated_VT_VP'],\n",
    "     'Downregulated Significant (VT/VP)': average_properties_series['Experiment_Downregulated_VT_VP']},\n",
    "    'Comparison of Peptides (VT/VP)',\n",
    "    'comparison_peptide_properties',\n",
    "    RESULTS_DIR\n",
    ")\n",
    "\n",
    "print(\"\\nAll Analysis Complete! Check your results directory for summary CSVs and plots.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
